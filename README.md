# QueryHub - RAG-Based Text-to-SQL System

QueryHub is a Retrieval-Augmented Generation (RAG)-based text-to-SQL platform that enables users to securely connect cloud-hosted SQL datasets and interact with them via natural language queries. It automatically generates SQL, executes queries, and returns results as shareable datasets or interactive visualizations.

---

## ğŸ‘¥ Team Members

- Jay Vipin Jajoo
- Rohan Ojha
- Rahul Reddy Mandadi
- Abhinav Gadgil
- Ved Dipak Deore
- Ashwin Khairnar

---

## ğŸš€ Features

- **Natural Language Querying**: Convert plain English queries into accurate SQL/NoSQL commands
- **Real-Time Database Connectivity**: Securely connect to relational databases such as Google Cloud SQL, AWS RDS, and Azure SQL
- **Auto-Generated Visualizations**: Transform query results into dynamic Plotly-based charts
- **CSV Export**: Download query outputs as CSV files for offline analysis
- **Feedback Loop**: Users can refine charts and queries iteratively
- **Monitoring & Logging**: Track model performance, latency, visualization success, and system uptime

---

## ğŸ“‹ Table of Contents

1. [Quick Start](#-quick-start)
2. [Environment Variables](#-environment-variables)
3. [Data Pipeline Setup](#-data-pipeline-setup)
4. [Backend Setup](#-backend-setup)
5. [Frontend Setup](#-frontend-setup)
6. [Model Training Pipeline](#-model-training-and-evaluation-pipeline)
7. [CI/CD & Deployment Scripts](#-cicd--deployment-scripts)
8. [Architecture](#-architecture)
9. [Repository Structure](#-repository-structure)

---

## ğŸ Quick Start

### Prerequisites

- **Operating System**: Linux, macOS, or Windows with WSL2
- **Python**: 3.10
- **Docker & Docker Compose**
- **Git**
- **DVC** (for data versioning)
- **RAM**: Minimum 8GB (16GB recommended for parallel processing)
- **CPU**: Multi-core processor (pipeline uses 75% of cores)
- **Disk Space**: ~10GB for dataset and generated files
- **Google Cloud Platform** account (with appropriate permissions)

```bash
# Verify installations
python --version
docker --version
git --version
dvc version
```

### 1. Clone the Repository

```bash
git clone https://github.com/Northeastern-MLOps-Group-12/QueryHub.git
cd QueryHub
```

---

### 2. Environment Variables

Create a `.env` file in the `backend/` directory with the following variables:

```bash
# Database Configuration
DATABASE_URL=postgresql+pg8000://user:password@host:port/database

# LLM Configuration
LLM_API_KEY=your_llm_api_key
MODEL=gemini
MODEL_NAME=gemini-2.5-flash
EMBD_MODEL_PROVIDER=google
EMBEDDING_MODEL=text-embedding-004

# Frontend Configuration
FRONTEND_ORIGIN=http://localhost:5173

# GCP Configuration
PROJECT_ID=your_gcp_project_id
GCS_BUCKET_NAME=your_gcs_bucket
GCS_VECTORSTORE_BUCKET_NAME=your_vectorstore_bucket
FIREBASE_DATABASE_ID=your_firebase_db_id

# Authentication
ACCESS_TOKEN_EXPIRE_MINUTES=30
SECRET_KEY=your_secret_key
ALGORITHM=HS256

# Application Mode
MODE=development

# LangSmith Tracing (Optional)
LANGSMITH_TRACING=true
LANGSMITH_ENDPOINT=https://api.smith.langchain.com
LANGSMITH_API_KEY=your_langsmith_api_key
LANGSMITH_PROJECT=your_project_name

# OpenAI (if using GPT models)
OPENAI_API_KEY=your_openai_api_key
```

### 3. Authenticate Google Cloud
```bash
# Login to Google Cloud
gcloud auth login

# Set your project
gcloud config set project your_gcp_project_id

# Authenticate application default credentials (for local development)
gcloud auth application-default login
```

### 4. Start Backend
```bash
cd backend
docker compose up
```
Navigate to `http://localhost:8081` to access the backend Swagger.

### 5. Start Data Pipeline
```bash
cd data-pipeline
docker compose up
```
Navigate to `http://localhost:8080` to access the data pipelines.
Navigate to `http://localhost:3001` to access the Grafana monitoring dashboard.

### 6. Start Frontend
```bash
cd frontend
docker compose up
```
Navigate to `http://localhost:5173` to access the frontend interface.

---

## ğŸ”„ Data Pipeline Setup

The data pipeline uses Apache Airflow to orchestrate ETL workflows, model training, and retraining DAGs.

### Data Pipeline Variables
```json
{
  "alert_email": "<mail_to_send_alerts>",
  "gcp_evaluation_output_csv": "gs://<bucket_name>/output_data/",
  "gcp_processed_data_path": "gs://<bucket_name>/processed_datasets",
  "gcp_project": "queryhub-473901",
  "gcp_region": "us-east1",
  "gcs_bias_and_syntax_validation_output": "gs://<bucket_name>/bias_and_syntax_validation/",
  "gcs_bucket_name": "gs://<bucket_name>",
  "gcs_registered_models": "gs://<bucket_name>/registered_models",
  "gcs_staging_bucket": "gs://<bucket_name>/staging_bucket",
  "num_train_epochs": "1",
  "service_account": "<your_service_account_email>",
  "serving_container_image_uri": "<your_image_on_artifiact_registry>",
  "train_samples": "1",
  "val_samples": "1",
  "vertex_ai_eval_gpu_type": "NVIDIA_L4",
  "vertex_ai_eval_machine_type": "g2-standard-4",
  "vertex_ai_train_gpu_type": "NVIDIA_L4",
  "vertex_ai_training_image_uri": "<your_image_for_training_n_eval_scripts>",
  "vertex_ai_train_machine_type": "g2-standard-4"
}
```

### DAGs Overview

| DAG | Description |
|-----|-------------|
| **Data Pipeline DAG** | Handles data ingestion, SQL validation, duplicate removal, bias detection/mitigation, and schema validation |
| **Retraining DAG** | Monitors data drift based on SQL complexity distribution and triggers model retraining when thresholds are exceeded |

### Airflow ETL Pipeline Components

| Component | Description |
|-----------|-------------|
| Data Ingestion | Ingests synthetic SQL datasets using GretelAI and custom scripts |
| SQL Validation | Validates SQL syntax with `sqlglot` |
| Duplicate Removal | Drops duplicate synthetic queries |
| **Bias Detection** | Detects underrepresentation in SQL types (JOIN, CTE, Aggregations) |
| **Bias Mitigation** | Generates synthetic SQL to rebalance dataset |
| Schema Validation | Ensures dataset follows strict structure |
| Notifications | Sends email alerts for bias or task failures |

### Setup Instructions

#### 1. Navigate to Data Pipeline Directory

```bash
cd data-pipeline
```

#### 2. Set Up Python Environment (Optional for local development)

```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

#### 3. Data Versioning with DVC

##### Initialize DVC
```bash
dvc init
```

##### Configure Remote Storage (GCS)
```bash
dvc remote add -d myremote gs://my-bucket/data-pipeline
dvc remote modify myremote credentialpath ~/.config/gcloud/credentials.json
```

##### Configure Remote Storage (AWS S3 - Alternative)
```bash
dvc remote add -d myremote s3://my-bucket/data-pipeline
dvc remote modify myremote access_key_id YOUR_ACCESS_KEY
dvc remote modify myremote secret_access_key YOUR_SECRET_KEY
```

##### Track Data Directory
```bash
dvc add data/
git add data.dvc .gitignore
git commit -m "Track data with DVC"
dvc push
```

#### 4. Configure Airflow

```bash
# Create necessary folders
mkdir ./logs ./plugins ./config

# Initialize Airflow
docker compose run airflow-cli airflow config list

# Initialize Airflow DB
docker compose up airflow-init

# Start Docker Services
docker compose up -d
```

#### 5. Configure SMTP for Email Alerts

Edit `docker-compose.yaml`:

```yaml
environment:
  AIRFLOW__SMTP__SMTP_HOST: smtp.gmail.com
  AIRFLOW__SMTP__SMTP_PORT: 587
  AIRFLOW__SMTP__SMTP_USER: your-email@gmail.com
  AIRFLOW__SMTP__SMTP_PASSWORD: your-app-password
  AIRFLOW__SMTP__SMTP_MAIL_FROM: your-email@gmail.com
```

**Note**: For Gmail, use an [App Password](https://support.google.com/accounts/answer/185833).

#### 6. Access Airflow UI

Navigate to `http://localhost:8080`

Login credentials:
- **Username**: airflow
- **Password**: airflow

#### 7. Configure Airflow Variables

Go to **Admin â†’ Variables** and import `data-pipeline/variables.json`, or set manually:

| Variable | Description |
|----------|-------------|
| `alert_email` | Email address for pipeline failure alerts |
| `gcp_evaluation_output_csv` | GCS path for evaluation output CSVs |
| `gcp_processed_data_path` | GCS path for saving processed data |
| `gcp_project` | GCP Project ID |
| `gcp_region` | GCP Region (e.g., `us-central1`) |
| `gcs_bias_and_syntax_validation_output` | GCS path for bias/syntax validation outputs |
| `gcs_bucket_name` | GCS bucket name (use `gs://<bucket_name>` format) |
| `gcs_registered_models` | GCS path to trained model files |
| `gcs_staging_bucket` | Vertex AI staging bucket |
| `num_train_epochs` | Number of training epochs |
| `service_account` | GCP Service Account email |
| `serving_container_image_uri` | Docker image URI for model serving |
| `train_samples` | Number of training samples |
| `val_samples` | Number of validation samples |
| `vertex_ai_eval_gpu_type` | GPU type for evaluation (e.g., `NVIDIA_TESLA_T4`) |
| `vertex_ai_eval_machine_type` | Machine type for evaluation |
| `vertex_ai_train_gpu_type` | GPU type for training |
| `vertex_ai_training_image_uri` | Docker image URI for training/eval scripts |
| `vertex_ai_train_machine_type` | Machine type for training |

#### 8. Run the Pipeline

1. In the Airflow UI, find the DAG: `data_pipeline_with_synthetic_v1_schema_validation`
2. Toggle the DAG to **ON**
3. Click **Trigger DAG** to start execution
4. Monitor progress in the **Graph View** or **Gantt Chart**

#### 9. DVC Workflow (After Pipeline Execution)

```bash
# Track new data files
dvc add data/

# Commit DVC files
git add data.dvc
git commit -m "Update dataset after pipeline run"

# Push data to remote
dvc push
git push
```

#### 10. Reproduce on Another Machine

```bash
git clone https://github.com/Northeastern-MLOps-Group-12/QueryHub.git
cd QueryHub/data-pipeline
dvc pull
ls data/
```

---

## ğŸ–¥ï¸ Backend Setup

The backend is built with FastAPI and provides REST APIs for database connections, query execution, and visualization generation.

### Setup Instructions

```bash
cd backend
docker compose up -d
```

### Access the API

Navigate to `http://localhost:8081` to open the Swagger UI with all available API endpoints.

### Key API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/connect/addConnection` | POST | Add a new database connection |
| `/connect/connections` | GET | List all connections |
| `/query/execute` | POST | Execute a natural language query |
| `/health` | GET | Health check endpoint |

### LangGraph Workflow

The application uses a **LangGraph workflow** with two primary nodes:

**1. `save_creds`**
- **Purpose**: Securely stores database credentials
- **Process**: Validates and encrypts connection parameters before storage

**2. `build_vector_store`**
- **Purpose**: Indexes database schema for intelligent querying
- **Process**:
  1. Retrieves the database schema using stored credentials
  2. Generates natural language descriptions of tables, columns, and relationships using LLM
  3. Chunks and embeds the schema information
  4. Stores embeddings in **ChromaDB** vector database for semantic search

---

## ğŸ¨ Frontend Setup

The frontend is built with React/TypeScript and provides an intuitive interface for database querying and visualization.

### Setup Instructions

```bash
cd frontend
docker compose up -d
```

### Access the Application

Navigate to `http://localhost:5173` to access the QueryHub interface.

---

## âš™ï¸ Model Training and Evaluation Pipeline

### Model Overview

| Component | Details |
|-----------|----------|
| Base Model | `t5-large-lm-adapt` (Spider SQL) |
| Fine-Tuning | LoRA (Low-Rank Adaptation) |
| Dataset | Custom SQL dataset (GretelAI + synthetic queries) |
| Training Metadata | Includes query complexity & domain context |
| Versioning | Managed via **DVC** |
| Evaluation | Execution Accuracy (EX) + Logical Form Match (EM) |

### Required GCP Resources

1. **GCS Buckets**: Training/Testing/Validation data, Model Artifacts, Prediction results, Evaluation results, Validation outputs
2. **Vertex AI**: GPU Quota (ideally 2), Enabled API, Service account with permissions, Model Registry enabled

### Build & Push Vertex AI Training Image

From `model_fine_tuning/vertex_ai_image` directory:

```bash
# Build the Docker Image
docker build --platform linux/amd64 -t your-region-docker.pkg.dev/your-project/your-artifact/your-image-name:your-image-tag .

# Push to Artifact Registry
docker push your-region-docker.pkg.dev/your-project/your-artifact/your-image-name:your-image-tag
```

### Trigger Training Pipeline

**From Airflow UI:**
1. Navigate to DAGs
2. Locate `vertex_ai_model_training_pipeline`
3. Click **Trigger DAG**

**From CLI:**
```bash
airflow dags trigger vertex_ai_model_training_pipeline
```

---

## ğŸš¢ CI/CD & Deployment Scripts

QueryHub uses GitHub Actions for continuous integration and deployment. The following workflows are configured in `.github/workflows/`:

| Workflow | File | Description |
|----------|------|-------------|
| **Airflow Deployment** | `airflow-cicd.yml` | Deploys Data and Model Training pipeline to VM |
| **Verte AI Scripts** | `build-vertex-ai-image.yml` | Pushes Model Training and Evaluation Script to Artifact Registry |
| **Deploy Backend** | `deploy-backend.yml` | Deploys the Cackend to Google Cloud Run |
| **Deploy Monitoring** | `deploy-monitoring.yml` | Pushes Monitoring code to VM and deploys Grafana/Prometheus dashboards |
| **Deploy Frontend** | `frontend-deploy.yml` | Deploys the Frontend to Google Cloud Run |
| **Run Tests** | `run_tests.yml` | Executes unit tests on every Push and PR |
| **Trigger Data Pipeline** | `trigger-data-pipeline.yml` | Triggers Data Pipeline DAG daily at 10 AM EST |

### Deployment Architecture

- **Backend**: Deployed to Cloud Run (containerized FastAPI service)
- **Frontend**: Deployed to Cloud Run (containerized React app)
- **Data Pipeline**: Runs on a GCP VM with Docker (Airflow + DAGs)
- **Model Training**: Runs on Vertex AI
- **Monitoring**: Grafana and Prometheus deployed on a dedicated VM

### Verifying Deployment

To verify the Airflow VM is running correctly:

```bash
# SSH into your VM
gcloud compute ssh <vm-name> --zone <zone>

# Check running containers
docker ps
```

You should see Airflow webserver, scheduler, and worker containers running.

---

## ğŸ—ï¸ Architecture

### Backend Flowchart

![Backend Architecture](https://lucid.app/publicSegments/view/967cb8f0-2b53-499e-94b2-ee26074eb6f5/image.png)

### Frontend Flowchart

![Frontend Flow](https://lucid.app/publicSegments/view/91d4e32f-6dbd-4131-9993-55b6a51896e3/image.png)

### Deployment Architecture

![Overall Architecture](https://lucid.app/publicSegments/view/3bb3a15f-5945-44b9-8498-473e13a5fc95/image.png)

---

## ğŸ“‚ Repository Structure

```
QueryHub/
â”œâ”€â”€ .github/workflows/                              # GitHub Actions Workflows
â”‚   â”œâ”€â”€ airflow-cicd.yml                            # Deploy airflow
â”‚   â”œâ”€â”€ build-vertex-ai-image.yml                   # Push training image to artifact registry
â”‚   â”œâ”€â”€ deploy-backend.yml                          # Deploy backend
â”‚   â”œâ”€â”€ deploy-monitoring.yml                       # Deploy monitoring
â”‚   â”œâ”€â”€ frontend-deploy.yml                         # Deploy frontend
â”‚   â”œâ”€â”€ run_tests.yml                               # CI/CD workflow to run tests
â”‚   â””â”€â”€ trigger-data-pipeline.yml                   # Triggers data pipeline daily
â”‚
â”œâ”€â”€ .vscode/
â”‚   â””â”€â”€ settings.json                               # Internal Settings
â”‚
â”œâ”€â”€ agents/                                         # LLM agent logic
â”‚   â”œâ”€â”€ load_data_to_vector/
â”‚   â”‚   â”œâ”€â”€ graph.py                                # Defines workflow graph for agents
â”‚   â”‚   â”œâ”€â”€ load_creds_to_vectordb.py               # Saves DB creds & builds vector store
â”‚   â”‚   â””â”€â”€ state.py                                # Pydantic models for agent state
â”‚   â”œâ”€â”€ nl_to_data_viz/
â”‚   â”‚   â”œâ”€â”€ database_selector.py
â”‚   â”‚   â”œâ”€â”€ generate_sql_query.py
â”‚   â”‚   â”œâ”€â”€ graph.py
â”‚   â”‚   â”œâ”€â”€ guardrails.py
â”‚   â”‚   â”œâ”€â”€ query_result_saver.py
â”‚   â”‚   â”œâ”€â”€ sql_complexity_analyzer.py
â”‚   â”‚   â”œâ”€â”€ sql_runner.py
â”‚   â”‚   â”œâ”€â”€ state.py
â”‚   â”‚   â””â”€â”€ test_guardrails.py
â”‚   â”œâ”€â”€ update_data_in_vector/
â”‚   â”‚   â”œâ”€â”€ graph.py
â”‚   â”‚   â”œâ”€â”€ state.py
â”‚   â”‚   â””â”€â”€ update_creds_in_vectordb.py
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ base_agent.py                               # Base wrapper for chat models
â”‚
â”œâ”€â”€ backend/                                        # FastAPI backend
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ chat_model.py 
â”‚   â”‚   â”œâ”€â”€ chat_request.py 
â”‚   â”‚   â”œâ”€â”€ connector_request.py                    # Pydantic models for API requests
â”‚   â”‚   â”œâ”€â”€ signin_request.py 
â”‚   â”‚   â”œâ”€â”€ signin_response.py 
â”‚   â”‚   â”œâ”€â”€ signup_request.py 
â”‚   â”‚   â”œâ”€â”€ tokendata.py
â”‚   â”‚   â””â”€â”€ user_response.py
â”‚   â”œâ”€â”€ monitoring/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â””â”€â”€ monitoring.json
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ __init__.py 
â”‚   â”‚   â”œâ”€â”€ agent_utils.py 
â”‚   â”‚   â”œâ”€â”€ chat_utils.py
â”‚   â”‚   â”œâ”€â”€ connectors_api_utils.py 
â”‚   â”‚   â”œâ”€â”€ user_api_utils.py 
â”‚   â”‚   â”œâ”€â”€ user_security.py 
â”‚   â”‚   â””â”€â”€ vectorstore_gcs.py
â”‚   â”œâ”€â”€ __init__
â”‚   â”œâ”€â”€ chat_api.py
â”‚   â”œâ”€â”€ connectors_api.py     # FastAPI routes for database connectors
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ main.py    
â”‚   â”œâ”€â”€ prometheus.yml
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ user_api.py
â”‚
â”œâ”€â”€ connectors/                                     # Database connector service
â”‚   â”œâ”€â”€ engines/
â”‚   â”‚   â”œâ”€â”€ mysql/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ mysql_connector.py
â”‚   â”‚   â””â”€â”€ postgres/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ postgres_connector.py
â”‚   â”œâ”€â”€ __init__.py                           
â”‚   â”œâ”€â”€ base_connector.py                           # Abstract base class for connectors
â”‚   â”œâ”€â”€ connector.py                                # Factory to instantiate connectors
â”‚   â””â”€ README.md                               
â”‚
â”œâ”€â”€ data-pipeline/                                  # Data pipeline and ETL DAGs
â”‚   â”œâ”€â”€ .dvc/
â”‚   â”‚   â”œâ”€â”€ config                          
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ model_scripts/                          # Model Training and Evaluation
â”‚   â”‚   â”‚   â”œâ”€â”€ bias_detection.py
â”‚   â”‚   â”‚   â”œâ”€â”€ dag_experiment_utils.py
â”‚   â”‚   â”‚   â”œâ”€â”€ model_deployment.py
â”‚   â”‚   â”‚   â”œâ”€â”€ model_eval_job_launcher.py
â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”‚   â”œâ”€â”€ retrain_model.py
â”‚   â”‚   â”‚   â”œâ”€â”€ syntax_validation.py
â”‚   â”‚   â”‚   â””â”€â”€ train_utils.py
â”‚   â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”‚   â”œâ”€â”€ DataGenData/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ DomainData/                     # Domain-specific generators
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Ecommerce.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Education.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Finance.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Gaming.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Healthcare.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Hospitality.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Logistics.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Manufacturing.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ RealEstate.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Retail.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SocialMedia.py
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Templates/                      # SQL query templates
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ CTETemplates.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ MultipleJoinsTemplates.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SETTemplates.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ SubQueryTemplates.py
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ WindowFunctionTemplates.py
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ DataGenerator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ EmailContentGenerator.py
â”‚   â”‚   â”‚   â”œâ”€â”€ SQLValidator.py
â”‚   â”‚   â”‚   â””â”€â”€ test_utils.py
â”‚   â”‚   â”œâ”€â”€ data_pipeline_dag.py                    # Main Airflow DAG
â”‚   â”‚   â””â”€â”€ train_model_and_save.py                 # Model Training DAG
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â””â”€â”€ docker-entrypoint.sh               
â”‚   â”œâ”€â”€ tests/
â”‚   â”‚   â”œâ”€â”€ model_training_tests.py             
â”‚   â”‚   â””â”€â”€ test.py    
â”‚   â”œâ”€â”€ .dvcignore
â”‚   â”œâ”€â”€ data.dvc
â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ databases/                                      # Database access layer
â”‚   â”œâ”€â”€ cloudsql/
â”‚   â”‚   â”œâ”€â”€ models         
â”‚   â”‚   â”‚   â”œâ”€â”€ credentials.py
â”‚   â”‚   â”‚   â”œâ”€â”€ user.py
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ crud.py
â”‚   â”‚   â””â”€â”€ database.py
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ frontend/                                       # React frontend application
â”‚   â”œâ”€â”€ public/
â”‚   â”‚   â””â”€â”€ logo.png                                # Application logo
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ account/                                # Authentication pages
â”‚   â”‚   â”‚   â”œâ”€â”€ index.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ SignIn.tsx
â”‚   â”‚   â”‚   â””â”€â”€ SignUp.tsx
â”‚   â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”‚   â””â”€â”€ default-avatar.png                  # Default user avatar
â”‚   â”‚   â”œâ”€â”€ chat-interface/                         # Chat UI components
â”‚   â”‚   â”‚   â”œâ”€â”€ index.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ NewChatModal.css
â”‚   â”‚   â”‚   â””â”€â”€ NewChatModal.tsx
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ ProtectedRoute.tsx                  # Route authentication wrapper
â”‚   â”‚   â”œâ”€â”€ data/                                   # Static data and content
â”‚   â”‚   â”‚   â”œâ”€â”€ dbOptions.tsx
â”‚   â”‚   â”‚   â””â”€â”€ homeContent.tsx
â”‚   â”‚   â”œâ”€â”€ database/                               # Database connection components
â”‚   â”‚   â”‚   â”œâ”€â”€ ConnectedDatabases.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DatabaseConnection.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DatabaseDescription.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ DatabaseEditor.tsx
â”‚   â”‚   â”‚   â””â”€â”€ index.tsx
â”‚   â”‚   â”œâ”€â”€ home/
â”‚   â”‚   â”‚   â””â”€â”€ index.tsx                           # Landing/home page
â”‚   â”‚   â”œâ”€â”€ hooks/                                  # Custom React hooks
â”‚   â”‚   â”‚   â”œâ”€â”€ AuthProvider.tsx
â”‚   â”‚   â”‚   â””â”€â”€ useAuth.tsx
â”‚   â”‚   â”œâ”€â”€ services/                               # API service layer
â”‚   â”‚   â”‚   â”œâ”€â”€ api.ts                              # Base API configuration
â”‚   â”‚   â”‚   â”œâ”€â”€ authService.tsx                     # Authentication API calls
â”‚   â”‚   â”‚   â”œâ”€â”€ chatService.tsx                     # Chat-related API calls
â”‚   â”‚   â”‚   â””â”€â”€ databaseService.tsx                 # Database management API calls
â”‚   â”‚   â”œâ”€â”€ App.css                                 # Global app styles
â”‚   â”‚   â”œâ”€â”€ App.tsx                                 # Main app component
â”‚   â”‚   â”œâ”€â”€ Footer.tsx                              # Footer component
â”‚   â”‚   â”œâ”€â”€ index.css                               # Root styles
â”‚   â”‚   â”œâ”€â”€ main.tsx                                # Application entry point
â”‚   â”‚   â””â”€â”€ Navbar.tsx                              # Navigation bar component
â”‚   â”œâ”€â”€ .dockerignore                               # Docker ignore rules
â”‚   â”œâ”€â”€ .gitignore                                  # Git ignore rules
â”‚   â”œâ”€â”€ docker-compose.yml                          # Docker configuration for frontend
â”‚   â”œâ”€â”€ Dockerfile                                  # Docker image definition
â”‚   â”œâ”€â”€ eslint.config.js                            # ESLint configuration
â”‚   â”œâ”€â”€ index.html                                  # HTML entry point
â”‚   â”œâ”€â”€ nginx.conf                                  # Nginx server configuration
â”‚   â”œâ”€â”€ package-lock.json                           # Locked dependencies
â”‚   â”œâ”€â”€ package.json                                # NPM dependencies and scripts
â”‚   â”œâ”€â”€ README.md                                   # Frontend documentation
â”‚   â”œâ”€â”€ tsconfig.app.json                           # TypeScript config for app
â”‚   â”œâ”€â”€ tsconfig.json                               # Base TypeScript configuration
â”‚   â”œâ”€â”€ tsconfig.node.json                          # TypeScript config for Node
â”‚   â””â”€â”€ vite.config.ts                              # Vite build configuration
â”‚
â”œâ”€â”€ model_fine_tuning/                              # Fine-tuning experiments and research
â”‚   â”œâ”€â”€ FT_NoteBooks/                               # Jupyter notebooks for fine-tuning
â”‚   â”‚   â”œâ”€â”€ QH_FT_Sensitivity.ipynb                 # Sensitivity analysis experiments
â”‚   â”‚   â”œâ”€â”€ QH_FT_T1.ipynb                          # Fine-tuning trial 1
â”‚   â”‚   â”œâ”€â”€ QH_FT_T2.ipynb                          # Fine-tuning trial 2
â”‚   â”‚   â””â”€â”€ QH_FT_T3.ipynb                          # Fine-tuning trial 3
â”‚   â””â”€â”€ vertex_ai_image/                            # Vertex AI custom training
â”‚       â”œâ”€â”€ Dockerfile                              # Custom training container
â”‚       â”œâ”€â”€ experiment_utils.py                     # Experiment utilities and helpers
â”‚       â”œâ”€â”€ model_eval.py                           # Model evaluation scripts
â”‚       â”œâ”€â”€ README.md                               # Vertex AI training documentation
â”‚       â”œâ”€â”€ requirements.txt                        # Python dependencies
â”‚       â””â”€â”€ train.py                                # Training script
â”‚
â”œâ”€â”€ tests/                                          # Test suite
â”‚   â”œâ”€â”€ agents/                                     # Agent component tests
â”‚   â”‚   â”œâ”€â”€ test_base_agent.py                      # Base agent functionality tests
â”‚   â”‚   â”œâ”€â”€ test_graph.py                           # Graph agent tests
â”‚   â”‚   â”œâ”€â”€ test_load_creds_to_vectordb.py          # Credential loading tests
â”‚   â”‚   â””â”€â”€ test_state.py                           # State management tests
â”‚   â”œâ”€â”€ backend/                                    # Backend API tests
â”‚   â”‚   â””â”€â”€ test_connectors_api.py                  # Connector API endpoint tests
â”‚   â”œâ”€â”€ connectors/    
â”‚   â”‚   â””â”€â”€ test_connectors.py                      # Database connector tests
â”‚   â”œâ”€â”€ conftest.py                                 # Pytest configuration and fixtures
â”‚   â””â”€â”€ requirements.txt                            # Test dependencies
â”œâ”€â”€ vectorstore/                                    # ChromaDB vector store integration
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ chroma_vector_store.py
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## ğŸ¥ Video Demo
- [QueryHub Demo](https://northeastern-my.sharepoint.com/:f:/g/personal/deore_v_northeastern_edu/IgASnWrcp33oRJ9C-89fnrx3AcSS7gJxhJezJU_nV01W4UA?e=puKjrH)

---

## ğŸ“„ Documentation

- [Scoping Document](https://docs.google.com/document/d/1Iblflv-p4wUgzQoSpWiBj2JXwwROFsZgEVZwYK-Z9Hs/edit?usp=sharing)
- [Model Development Document](https://docs.google.com/document/d/1D5nyl2Pb45JF5NJGTn9cwV6xBmpFTchtbwbQRXK_C5E/edit?usp=sharing)
- [Data Pipeline, Errors + Graceful Failures, User Needs + Defining Success, Data Drift, Model Retraining, Monitoring](https://northeastern-my.sharepoint.com/:f:/g/personal/deore_v_northeastern_edu/IgASnWrcp33oRJ9C-89fnrx3AcSS7gJxhJezJU_nV01W4UA?e=puKjrH)

---

## ğŸ“ License

This project is developed as part of the MLOps curriculum at Northeastern University.