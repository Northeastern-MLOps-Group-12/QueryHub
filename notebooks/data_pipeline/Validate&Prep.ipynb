{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Notebook 2: Validate & Preprocess for T5\n",
        "# !pip install transformers sqlparse"
      ],
      "metadata": {
        "id": "VVIXPDx9IvOk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sqlparse\n",
        "from transformers import AutoTokenizer\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "X-CTAhrBIzC3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "train_df = pd.read_pickle('train_raw.pkl')\n",
        "test_df = pd.read_pickle('test_raw.pkl')\n",
        "\n",
        "# SQL validation\n",
        "def validate_sql(sql):\n",
        "    try:\n",
        "        parsed = sqlparse.parse(sql)\n",
        "        return len(parsed) > 0 and len(parsed[0].tokens) > 0\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "print(\"Validating SQL...\")\n",
        "train_df = train_df.dropna(subset=['sql_prompt', 'sql'])\n",
        "\n",
        "# Create a boolean column first, then filter\n",
        "train_df['sql_valid'] = train_df['sql'].apply(validate_sql)\n",
        "train_df = train_df[train_df['sql_valid'] == True].drop(columns=['sql_valid'])\n",
        "\n",
        "test_df = test_df.dropna(subset=['sql_prompt', 'sql'])\n",
        "test_df['sql_valid'] = test_df['sql'].apply(validate_sql)\n",
        "test_df = test_df[test_df['sql_valid'] == True].drop(columns=['sql_valid'])\n",
        "\n",
        "print(f\"After validation - Train: {len(train_df)}, Test: {len(test_df)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Vy3c9ReI1F0",
        "outputId": "5c105345-f3bf-427b-e21e-170c7f98c994"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validating SQL...\n",
            "After validation - Train: 100000, Test: 5851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nsohjiw9IgjY"
      },
      "outputs": [],
      "source": [
        "# T5 formatting\n",
        "def format_for_t5(row):\n",
        "    if pd.notna(row['sql_context']) and row['sql_context'].strip():\n",
        "        return f\"translate English to SQL: {row['sql_prompt']} context: {row['sql_context']}\"\n",
        "    return f\"translate English to SQL: {row['sql_prompt']}\"\n",
        "\n",
        "def clean_sql(sql):\n",
        "    try:\n",
        "        return sqlparse.format(sql, reindent=False, keyword_case='upper', strip_comments=True).strip()\n",
        "    except:\n",
        "        return ' '.join(sql.split())  # Fallback to simple cleaning\n",
        "\n",
        "train_df['input_text'] = train_df.apply(format_for_t5, axis=1)\n",
        "train_df['target_text'] = train_df['sql'].apply(clean_sql)\n",
        "test_df['input_text'] = test_df.apply(format_for_t5, axis=1)\n",
        "test_df['target_text'] = test_df['sql'].apply(clean_sql)\n",
        "\n",
        "# Token analysis\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gaussalgo/T5-LM-Large-text2sql-spider\")\n",
        "sample = train_df.sample(min(1000, len(train_df)))\n",
        "\n",
        "input_lens = [len(tokenizer.encode(text)) for text in sample['input_text']]\n",
        "output_lens = [len(tokenizer.encode(text)) for text in sample['target_text']]\n",
        "\n",
        "print(f\"\\nToken stats:\")\n",
        "print(f\"Input: mean={np.mean(input_lens):.0f}, max={max(input_lens)}, >512={sum(l>512 for l in input_lens)}\")\n",
        "print(f\"Output: mean={np.mean(output_lens):.0f}, max={max(output_lens)}, >512={sum(l>512 for l in output_lens)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Token stats:\n",
        "Input: mean=141, max=487, >512=0\n",
        "Output: mean=52, max=216, >512=0"
      ],
      "metadata": {
        "id": "vBpjbx1UmtkU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save\n",
        "train_df[['input_text', 'target_text', 'domain', 'sql_complexity']].to_pickle('train_processed.pkl')\n",
        "test_df[['input_text', 'target_text', 'domain', 'sql_complexity']].to_pickle('test_processed.pkl')\n",
        "print(\"Preprocessed and saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OhIBg6FI7GL",
        "outputId": "1a4d0488-a67e-4a33-e1ce-3d7aabd84a98"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed and saved\n"
          ]
        }
      ]
    }
  ]
}