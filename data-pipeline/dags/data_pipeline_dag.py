from airflow import DAG
from airflow.operators.python import PythonOperator, BranchPythonOperator
from datetime import datetime, timedelta
from utils.EmailContentGenerator import notify_task_failure
from utils.SQLValidator import _validate_single_sql
import logging
import json
import os
import pandas as pd
from google.cloud import storage
import io
from airflow.models import Variable
from airflow.providers.google.cloud.hooks.gcs import GCSHook
from airflow.providers.standard.operators.trigger_dagrun import TriggerDagRunOperator

# ============================================================================
# DAG DEFAULT ARGS WITH FAILURE NOTIFICATION
# ============================================================================

default_args = {
    'owner': 'queryhub',
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
    'on_failure_callback': notify_task_failure,  # Send email on any task failure
}

def run_pytest_tests(**context):
    """Run pytest tests before starting the pipeline"""
    import subprocess
    import logging
    import sys
    
    logging.info("ðŸ§ª RUNNING PRE-PIPELINE TESTS")
    
    test_file = '/opt/airflow/tests/test.py'
    
    try:
        result = subprocess.run(
            [sys.executable, '-m', 'pytest', test_file, '-v'],
            capture_output=True,
            text=True,
            timeout=60
        )
        
        logging.info(result.stdout)
        
        if result.returncode == 0:
            logging.info("âœ… ALL TESTS PASSED")
            return {'status': 'success'}
        else:
            logging.error("âŒ TESTS FAILED")
            raise Exception("Pre-pipeline tests failed!")
            
    except Exception as e:
        logging.error(f"Error: {str(e)}")
        raise

def load_data():
    from datasets import load_dataset
    import pandas as pd
    
    dataset = load_dataset("gretelai/synthetic_text_to_sql")
    train_df = pd.DataFrame(dataset['train'])
    test_df = pd.DataFrame(dataset['test'])
    
    train_df.to_pickle('/tmp/train_raw.pkl')
    test_df.to_pickle('/tmp/test_raw.pkl')
    
    logging.info(f"Loaded {len(train_df)} train, {len(test_df)} test")
    return {'train': len(train_df), 'test': len(test_df)}

def load_data_from_gcs(**context):
    """Load train and test data from the LATEST folder in GCS based on creation time
    
    Expected GCS structure:
    gs://text-to-sql-dataset-queryhub/raw_data/{folder_name}/train.csv
    gs://text-to-sql-dataset-queryhub/raw_data/{folder_name}/test.csv
    
    The function automatically finds the most recently created folder using GCS metadata.
    """
    
    # GCS Configuration
    bucket_name = "text-to-sql-dataset-queryhub"
    project_id = "queryhub-459602"
    raw_data_prefix = "raw_data/"
    
    logging.info("=" * 60)
    logging.info("LOADING DATA FROM GCS")
    logging.info("=" * 60)
    logging.info(f"Project: {project_id}")
    logging.info(f"Bucket: {bucket_name}")
    logging.info(f"Prefix: {raw_data_prefix}")
    
    # Initialize GCS client
    client = storage.Client(project=project_id)
    bucket = client.bucket(bucket_name)
    
    logging.info("âœ… Connected to GCS")
    
    # Find all folders and their creation times
    logging.info("Searching for data folders...")
    
    folder_creation_times = {}
    
    for blob in bucket.list_blobs(prefix=raw_data_prefix):
        # Extract folder name from path like "raw_data/20251120_035946/train.csv"
        path_parts = blob.name[len(raw_data_prefix):].split('/')
        
        if len(path_parts) >= 2 and path_parts[0]:
            folder_name = path_parts[0]
            
            # Track the earliest created_at time for each folder
            if folder_name not in folder_creation_times:
                folder_creation_times[folder_name] = blob.time_created
            else:
                if blob.time_created < folder_creation_times[folder_name]:
                    folder_creation_times[folder_name] = blob.time_created
    
    if not folder_creation_times:
        raise ValueError(
            f"No data folders found in gs://{bucket_name}/{raw_data_prefix}\n"
            f"Please upload train.csv and test.csv to a subfolder."
        )
    
    # Sort folders by creation time (most recent first)
    sorted_folders = sorted(
        folder_creation_times.items(), 
        key=lambda x: x[1], 
        reverse=True
    )
    
    latest_folder = sorted_folders[0][0]
    latest_created_at = sorted_folders[0][1]
    
    logging.info(f"\nFound {len(folder_creation_times)} folder(s):")
    for folder_name, created_at in sorted_folders[:5]:
        marker = " â† LATEST (using this)" if folder_name == latest_folder else ""
        logging.info(f"   - {folder_name} (created: {created_at}){marker}")
    if len(sorted_folders) > 5:
        logging.info(f"   ... and {len(sorted_folders) - 5} more")
    
    # Define paths to train and test files
    train_gcs_path = f"{raw_data_prefix}{latest_folder}/train.csv"
    test_gcs_path = f"{raw_data_prefix}{latest_folder}/test.csv"
    
    # Load train data
    logging.info(f"\nDownloading: gs://{bucket_name}/{train_gcs_path}")
    train_blob = bucket.blob(train_gcs_path)
    
    if not train_blob.exists():
        raise FileNotFoundError(
            f"train.csv not found in gs://{bucket_name}/{raw_data_prefix}{latest_folder}/"
        )
    
    train_content = train_blob.download_as_bytes()
    train_df = pd.read_csv(io.BytesIO(train_content))
    logging.info(f"âœ… Loaded train.csv: {len(train_df):,} rows")
    
    # Load test data
    logging.info(f"Downloading: gs://{bucket_name}/{test_gcs_path}")
    test_blob = bucket.blob(test_gcs_path)
    
    if not test_blob.exists():
        raise FileNotFoundError(
            f"test.csv not found in gs://{bucket_name}/{raw_data_prefix}{latest_folder}/"
        )
    
    test_content = test_blob.download_as_bytes()
    test_df = pd.read_csv(io.BytesIO(test_content))
    logging.info(f"âœ… Loaded test.csv: {len(test_df):,} rows")
    
    # Validate required columns
    required_columns = ['sql_prompt', 'sql_context', 'sql', 'sql_complexity']
    
    missing_train_cols = [col for col in required_columns if col not in train_df.columns]
    missing_test_cols = [col for col in required_columns if col not in test_df.columns]
    
    if missing_train_cols:
        raise ValueError(f"Missing columns in train.csv: {missing_train_cols}")
    if missing_test_cols:
        raise ValueError(f"Missing columns in test.csv: {missing_test_cols}")
    
    logging.info(f"âœ… Schema validated")
    
    # Save to pickle files for downstream tasks
    train_df.to_pickle('/tmp/train_raw.pkl')
    test_df.to_pickle('/tmp/test_raw.pkl')
    
    logging.info("\n" + "=" * 60)
    logging.info("DATA LOADING COMPLETE")
    logging.info("=" * 60)
    logging.info(f"Source: gs://{bucket_name}/{raw_data_prefix}{latest_folder}/")
    logging.info(f"Created at: {latest_created_at}")
    logging.info(f"Train: {len(train_df):,} | Test: {len(test_df):,}")
    
    # Push to XCom
    context['task_instance'].xcom_push(key='source_folder', value=latest_folder)
    context['task_instance'].xcom_push(key='source_created_at', value=str(latest_created_at))
    
    return {
        'train': len(train_df), 
        'test': len(test_df),
        'source_folder': latest_folder
    }

def compare_datasets(**context):
    """
    Compare current pipeline dataset with the latest dataset from GCS.
    Calculates:
    1. Percentage change in distribution of sql_complexity
    2. SQL query length change per sql_complexity
    
    Triggers retraining DAG if:
    - Any sql_complexity distribution change > 10%
    - Any sql_length mean change > 5%
    """
    import pandas as pd
    from google.cloud import storage
    import io
    import json
    import os
    
    # GCS Configuration
    bucket_name = "text-to-sql-dataset-queryhub"
    project_id = "queryhub-459602"
    processed_prefix = "processed_datasets/"
    
    # Thresholds for triggering retraining
    COMPLEXITY_CHANGE_THRESHOLD = 10.0  # percent
    SQL_LENGTH_CHANGE_THRESHOLD = 5.0   # percent
    
    logging.info("=" * 60)
    logging.info("COMPARING DATASETS")
    logging.info("=" * 60)
    
    # -------------------------
    # Load current pipeline dataset
    # -------------------------
    current_path = "/opt/airflow/data/train.csv"
    
    if not os.path.exists(current_path):
        raise FileNotFoundError(f"Current train.csv not found at {current_path}")
    
    current_df = pd.read_csv(current_path)
    logging.info(f"âœ… Loaded current dataset: {len(current_df):,} rows")
    
    # -------------------------
    # Fetch latest dataset from GCS
    # -------------------------
    client = storage.Client(project=project_id)
    bucket = client.bucket(bucket_name)
    
    logging.info(f"Searching for latest folder in gs://{bucket_name}/{processed_prefix}")
    
    # Find all folders and their creation times
    folder_creation_times = {}
    
    for blob in bucket.list_blobs(prefix=processed_prefix):
        path_parts = blob.name[len(processed_prefix):].split('/')
        
        if len(path_parts) >= 2 and path_parts[0] and path_parts[0] != "latest.json":
            folder_name = path_parts[0]
            
            if folder_name not in folder_creation_times:
                folder_creation_times[folder_name] = blob.time_created
            else:
                if blob.time_created < folder_creation_times[folder_name]:
                    folder_creation_times[folder_name] = blob.time_created
    
    if not folder_creation_times:
        logging.warning("No previous datasets found in GCS. Skipping comparison.")
        context['task_instance'].xcom_push(key='data_drift_detected', value=False)
        return {"comparison_skipped": True, "reason": "No previous datasets in GCS"}
    
    # Sort by creation time and get latest
    sorted_folders = sorted(
        folder_creation_times.items(),
        key=lambda x: x[1],
        reverse=True
    )
    
    latest_folder = sorted_folders[0][0]
    latest_created_at = sorted_folders[0][1]
    
    logging.info(f"âœ… Found latest GCS folder: {latest_folder} (created: {latest_created_at})")
    
    # Download previous train.csv
    previous_gcs_path = f"{processed_prefix}{latest_folder}/train.csv"
    previous_blob = bucket.blob(previous_gcs_path)
    
    if not previous_blob.exists():
        logging.warning(f"train.csv not found in {latest_folder}. Skipping comparison.")
        context['task_instance'].xcom_push(key='data_drift_detected', value=False)
        return {"comparison_skipped": True, "reason": "train.csv not found in latest GCS folder"}
    
    previous_content = previous_blob.download_as_bytes()
    previous_df = pd.read_csv(io.BytesIO(previous_content))
    logging.info(f"âœ… Loaded previous dataset: {len(previous_df):,} rows")
    
    # -------------------------
    # Calculate SQL query length
    # -------------------------
    current_df['sql_length'] = current_df['sql'].str.len()
    previous_df['sql_length'] = previous_df['sql'].str.len()
    
    # Track trigger reasons
    trigger_reasons = []
    
    # -------------------------
    # 1. SQL Complexity Distribution Change
    # -------------------------
    logging.info("\nðŸ“Š SQL COMPLEXITY DISTRIBUTION COMPARISON")
    logging.info("-" * 50)
    
    current_complexity_dist = current_df['sql_complexity'].value_counts(normalize=True) * 100
    previous_complexity_dist = previous_df['sql_complexity'].value_counts(normalize=True) * 100
    
    # Get all complexity types
    all_complexities = set(current_complexity_dist.index) | set(previous_complexity_dist.index)
    
    complexity_comparison = []
    for complexity in sorted(all_complexities):
        current_pct = current_complexity_dist.get(complexity, 0)
        previous_pct = previous_complexity_dist.get(complexity, 0)
        
        if previous_pct > 0:
            pct_change = ((current_pct - previous_pct) / previous_pct) * 100
        else:
            pct_change = 100.0 if current_pct > 0 else 0.0
        
        complexity_comparison.append({
            'sql_complexity': complexity,
            'previous_pct': round(previous_pct, 2),
            'current_pct': round(current_pct, 2),
            'pct_point_change': round(current_pct - previous_pct, 2),
            'pct_change': round(pct_change, 2)
        })
        
        # Check threshold
        if abs(pct_change) > COMPLEXITY_CHANGE_THRESHOLD:
            trigger_reasons.append(f"sql_complexity '{complexity}' changed by {pct_change:+.2f}% (threshold: Â±{COMPLEXITY_CHANGE_THRESHOLD}%)")
        
        change_indicator = "â†‘" if pct_change > 0 else "â†“" if pct_change < 0 else "="
        threshold_marker = " âš ï¸ EXCEEDS THRESHOLD" if abs(pct_change) > COMPLEXITY_CHANGE_THRESHOLD else ""
        logging.info(f"   {complexity}: {previous_pct:.2f}% â†’ {current_pct:.2f}% ({change_indicator} {pct_change:+.2f}%){threshold_marker}")
    
    # -------------------------
    # 2. SQL Query Length Change per Complexity
    # -------------------------
    logging.info("\nðŸ“ SQL QUERY LENGTH COMPARISON (per complexity)")
    logging.info("-" * 50)
    
    current_length_stats = current_df.groupby('sql_complexity')['sql_length'].agg(['mean', 'median', 'std', 'min', 'max'])
    previous_length_stats = previous_df.groupby('sql_complexity')['sql_length'].agg(['mean', 'median', 'std', 'min', 'max'])
    
    length_comparison = []
    for complexity in sorted(all_complexities):
        current_stats = current_length_stats.loc[complexity] if complexity in current_length_stats.index else None
        previous_stats = previous_length_stats.loc[complexity] if complexity in previous_length_stats.index else None
        
        if current_stats is not None and previous_stats is not None:
            mean_change = ((current_stats['mean'] - previous_stats['mean']) / previous_stats['mean']) * 100
            median_change = ((current_stats['median'] - previous_stats['median']) / previous_stats['median']) * 100
            
            length_comparison.append({
                'sql_complexity': complexity,
                'previous_mean_length': round(previous_stats['mean'], 2),
                'current_mean_length': round(current_stats['mean'], 2),
                'mean_pct_change': round(mean_change, 2),
                'previous_median_length': round(previous_stats['median'], 2),
                'current_median_length': round(current_stats['median'], 2),
                'median_pct_change': round(median_change, 2)
            })
            
            # Check threshold for mean length change
            if abs(mean_change) > SQL_LENGTH_CHANGE_THRESHOLD:
                trigger_reasons.append(f"sql_length for '{complexity}' changed by {mean_change:+.2f}% (threshold: Â±{SQL_LENGTH_CHANGE_THRESHOLD}%)")
            
            change_indicator = "â†‘" if mean_change > 0 else "â†“" if mean_change < 0 else "="
            threshold_marker = " âš ï¸ EXCEEDS THRESHOLD" if abs(mean_change) > SQL_LENGTH_CHANGE_THRESHOLD else ""
            logging.info(f"   {complexity}:")
            logging.info(f"      Mean: {previous_stats['mean']:.1f} â†’ {current_stats['mean']:.1f} chars ({change_indicator} {mean_change:+.2f}%){threshold_marker}")
            logging.info(f"      Median: {previous_stats['median']:.1f} â†’ {current_stats['median']:.1f} chars ({change_indicator} {median_change:+.2f}%)")
    
    # -------------------------
    # 3. Overall Dataset Size Change
    # -------------------------
    logging.info("\nðŸ“ˆ OVERALL DATASET SIZE")
    logging.info("-" * 50)
    
    size_change = ((len(current_df) - len(previous_df)) / len(previous_df)) * 100
    logging.info(f"   Previous: {len(previous_df):,} rows")
    logging.info(f"   Current: {len(current_df):,} rows")
    logging.info(f"   Change: {size_change:+.2f}%")
    
    # -------------------------
    # Determine if data drift detected
    # -------------------------
    data_drift_detected = len(trigger_reasons) > 0
    
    logging.info("\n" + "=" * 60)
    if data_drift_detected:
        logging.info("ðŸš¨ DATA DRIFT DETECTED: YES")
        logging.info("=" * 60)
        logging.info(f"Found {len(trigger_reasons)} reason(s):")
        for reason in trigger_reasons:
            logging.info(f"   â€¢ {reason}")
        logging.info("\nâ†’ Will upload to GCS and trigger retraining DAG")
    else:
        logging.info("âœ… DATA DRIFT DETECTED: NO")
        logging.info("=" * 60)
        logging.info("All metrics within acceptable thresholds.")
        logging.info("â†’ Skipping upload and retraining")
    
    # -------------------------
    # Save comparison report
    # -------------------------
    comparison_report = {
        'timestamp': str(context.get('execution_date', datetime.now())),
        'previous_dataset': {
            'gcs_folder': latest_folder,
            'created_at': str(latest_created_at),
            'row_count': len(previous_df)
        },
        'current_dataset': {
            'path': current_path,
            'row_count': len(current_df)
        },
        'size_change_pct': round(size_change, 2),
        'complexity_distribution_comparison': complexity_comparison,
        'sql_length_comparison': length_comparison,
        'thresholds': {
            'complexity_change': COMPLEXITY_CHANGE_THRESHOLD,
            'sql_length_change': SQL_LENGTH_CHANGE_THRESHOLD
        },
        'data_drift_detected': data_drift_detected,
        'trigger_reasons': trigger_reasons
    }
    
    output_dir = "/opt/airflow/data"
    report_path = f"{output_dir}/dataset_comparison_report.json"
    
    with open(report_path, 'w') as f:
        json.dump(comparison_report, f, indent=2)
    
    logging.info(f"\nâœ… Comparison report saved to {report_path}")
    
    # Push to XCom for branching
    context['task_instance'].xcom_push(key='comparison_report', value=comparison_report)
    context['task_instance'].xcom_push(key='data_drift_detected', value=data_drift_detected)
    context['task_instance'].xcom_push(key='trigger_reasons', value=trigger_reasons)
    
    logging.info("\n" + "=" * 60)
    logging.info("DATASET COMPARISON COMPLETE")
    logging.info("=" * 60)
    
    return {
        'comparison_completed': True,
        'previous_folder': latest_folder,
        'previous_rows': len(previous_df),
        'current_rows': len(current_df),
        'size_change_pct': round(size_change, 2),
        'data_drift_detected': data_drift_detected,
        'trigger_reasons': trigger_reasons,
        'report_path': report_path
    }

def check_data_drift(**context):
    """
    Branch function to decide whether to upload and trigger retraining or skip.
    """
    data_drift_detected = context['task_instance'].xcom_pull(
        task_ids='compare_datasets',
        key='data_drift_detected'
    )
    
    if data_drift_detected:
        logging.info("ðŸš¨ Data drift detected â†’ Proceeding to upload and trigger retraining")
        return 'upload_to_gcp'
    else:
        logging.info("âœ… No data drift â†’ Skipping upload and retraining")
        return 'skip_upload'

def skip_upload(**context):
    """
    Log that upload and retraining were skipped due to no data drift.
    """
    logging.info("=" * 60)
    logging.info("UPLOAD & RETRAINING SKIPPED")
    logging.info("=" * 60)
    logging.info("No significant data drift detected.")
    logging.info("Dataset changes within acceptable thresholds:")
    logging.info("   â€¢ sql_complexity distribution change â‰¤ 10%")
    logging.info("   â€¢ sql_length mean change â‰¤ 5%")
    
    return {'skipped': True, 'reason': 'No data drift detected'}

def validate_sql():
    import pandas as pd
    import os
    from concurrent.futures import ProcessPoolExecutor, as_completed
    from multiprocessing import cpu_count
    
    train_df = pd.read_pickle('/tmp/train_raw.pkl')
    test_df = pd.read_pickle('/tmp/test_raw.pkl')
    
    # Track anomalies (invalid SQL queries)
    train_anomalies = []
    test_anomalies = []
    
    # Determine number of workers (use 75% of available CPUs)
    max_workers = max(1, int(cpu_count() * 0.75))
    logging.info(f"Using {max_workers} parallel workers for SQL validation")
    
    # Prepare data for parallel processing
    train_tasks = [(idx, sql, 'train') for idx, sql in enumerate(train_df['sql'])]
    test_tasks = [(idx, sql, 'test') for idx, sql in enumerate(test_df['sql'])]
    
    # Validate train data in parallel
    logging.info(f"Validating {len(train_tasks)} training SQL queries in parallel...")
    train_valid_mask = [False] * len(train_df)
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_task = {executor.submit(_validate_single_sql, task): task for task in train_tasks}
        
        for future in as_completed(future_to_task):
            task = future_to_task[future]
            try:
                idx, is_valid, error_msg, error_type = future.result()
                train_valid_mask[idx] = is_valid
                
                if not is_valid:
                    train_anomalies.append({
                        'source': 'train',
                        'index': idx,
                        'sql_query': task[1],
                        'error_message': error_msg,
                        'error_type': error_type
                    })
            except Exception as e:
                logging.error(f"Error processing train task {task[0]}: {e}")
                train_valid_mask[task[0]] = False
    
    train_valid_mask = pd.Series(train_valid_mask)
    
    # Validate test data in parallel
    logging.info(f"Validating {len(test_tasks)} test SQL queries in parallel...")
    test_valid_mask = [False] * len(test_df)
    
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        future_to_task = {executor.submit(_validate_single_sql, task): task for task in test_tasks}
        
        for future in as_completed(future_to_task):
            task = future_to_task[future]
            try:
                idx, is_valid, error_msg, error_type = future.result()
                test_valid_mask[idx] = is_valid
                
                if not is_valid:
                    test_anomalies.append({
                        'source': 'test',
                        'index': idx,
                        'sql_query': task[1],
                        'error_message': error_msg,
                        'error_type': error_type
                    })
            except Exception as e:
                logging.error(f"Error processing test task {task[0]}: {e}")
                test_valid_mask[task[0]] = False
    
    test_valid_mask = pd.Series(test_valid_mask)
    
    logging.info(f"Train: {train_valid_mask.sum()}/{len(train_df)} valid queries")
    logging.info(f"Test: {test_valid_mask.sum()}/{len(test_df)} valid queries")
    logging.info(f"Train anomalies detected: {len(train_anomalies)}")
    logging.info(f"Test anomalies detected: {len(test_anomalies)}")
    
    # Save anomalies report to CSV
    output_dir = '/opt/airflow/data'
    os.makedirs(output_dir, exist_ok=True)
    
    all_anomalies = train_anomalies + test_anomalies
    if all_anomalies:
        anomalies_df = pd.DataFrame(all_anomalies)
        anomalies_path = f'{output_dir}/sql_validation_anomalies.csv'
        anomalies_df.to_csv(anomalies_path, index=False)
        logging.info(f"âœ… Saved {len(all_anomalies)} anomalies to {anomalies_path}")
    else:
        logging.info("No anomalies detected - all SQL queries are valid")
    
    # Filter valid data
    train_df = train_df[train_valid_mask]
    test_df = test_df[test_valid_mask]
    
    train_df.to_pickle('/tmp/train_valid.pkl')
    test_df.to_pickle('/tmp/test_valid.pkl')
    
    return {
        'train_valid': len(train_df), 
        'test_valid': len(test_df),
        'train_anomalies': len(train_anomalies),
        'test_anomalies': len(test_anomalies),
        'total_anomalies': len(all_anomalies)
    }

def preprocess():
    import pandas as pd
    
    train_df = pd.read_pickle('/tmp/train_valid.pkl')
    test_df = pd.read_pickle('/tmp/test_valid.pkl')
    
    train_df.to_pickle('/tmp/train_preprocessed.pkl')
    test_df.to_pickle('/tmp/test_preprocessed.pkl')
    
    logging.info(f"Preprocessed {len(train_df)} train, {len(test_df)} test samples")
    return {'preprocessed': True}

def validate_raw_schema(**context):
    """Validate raw data schema and generate comprehensive baseline statistics"""
    import pandas as pd
    import numpy as np
    import json
    import os
    
    train_df = pd.read_pickle('/tmp/train_preprocessed.pkl')
    test_df = pd.read_pickle('/tmp/test_preprocessed.pkl')
    
    logging.info("=" * 60)
    logging.info("VALIDATING RAW DATA SCHEMA")
    logging.info("=" * 60)
    
    # Expected raw schema (before feature engineering)
    expected_schema = {
        'sql_prompt': 'object',  # string type in pandas
        'sql_context': 'object',
        'sql': 'object',
        'sql_complexity': 'object'
    }
    
    validation_errors = []
    
    # 1. Validate columns exist
    train_cols = set(train_df.columns.tolist())
    test_cols = set(test_df.columns.tolist())
    expected_cols = set(expected_schema.keys())
    
    missing_train = expected_cols - train_cols
    missing_test = expected_cols - test_cols
    
    if missing_train:
        validation_errors.append(f"Training data missing columns: {missing_train}")
    if missing_test:
        validation_errors.append(f"Test data missing columns: {missing_test}")
    
    # 2. Validate data types
    logging.info("\nðŸ” Validating Data Types...")
    for col, expected_dtype in expected_schema.items():
        if col in train_df.columns:
            actual_dtype = str(train_df[col].dtype)
            if actual_dtype != expected_dtype:
                validation_errors.append(
                    f"Column '{col}' in train has wrong dtype: expected {expected_dtype}, got {actual_dtype}"
                )
                logging.warning(f"âš ï¸ {col}: expected {expected_dtype}, got {actual_dtype}")
            else:
                logging.info(f"âœ… {col}: {actual_dtype}")
        
        if col in test_df.columns:
            actual_dtype = str(test_df[col].dtype)
            if actual_dtype != expected_dtype:
                validation_errors.append(
                    f"Column '{col}' in test has wrong dtype: expected {expected_dtype}, got {actual_dtype}"
                )
    
    # 3. Validate critical columns have no nulls
    critical_columns = ['sql_prompt', 'sql', 'sql_complexity']
    for col in critical_columns:
        train_nulls = train_df[col].isnull().sum()
        test_nulls = test_df[col].isnull().sum()
        
        if train_nulls > 0:
            validation_errors.append(f"Critical column '{col}' has {train_nulls} nulls in train")
        if test_nulls > 0:
            validation_errors.append(f"Critical column '{col}' has {test_nulls} nulls in test")
    
    if validation_errors:
        error_msg = "\n".join(validation_errors)
        logging.error(f"âŒ Schema validation failed:\n{error_msg}")
        raise ValueError(f"Schema validation failed:\n{error_msg}")
    
    logging.info(f"âœ… Schema validation passed - all checks successful")
    
    # Helper function for comprehensive statistics
    def calculate_text_statistics(series, column_name):
        """Calculate comprehensive statistics for text columns"""
        lengths = series.str.len()
        
        return {
            'count': int(series.notna().sum()),
            'null_count': int(series.isnull().sum()),
            'null_rate': float(series.isnull().mean()),
            'unique_count': int(series.nunique()),
            'length_stats': {
                'min': int(lengths.min()) if not lengths.empty else 0,
                'max': int(lengths.max()) if not lengths.empty else 0,
                'mean': float(lengths.mean()) if not lengths.empty else 0.0,
                'median': float(lengths.median()) if not lengths.empty else 0.0,
                'std': float(lengths.std()) if not lengths.empty else 0.0,
                'percentile_25': float(lengths.quantile(0.25)) if not lengths.empty else 0.0,
                'percentile_75': float(lengths.quantile(0.75)) if not lengths.empty else 0.0,
                'percentile_90': float(lengths.quantile(0.90)) if not lengths.empty else 0.0,
                'percentile_95': float(lengths.quantile(0.95)) if not lengths.empty else 0.0,
                'percentile_99': float(lengths.quantile(0.99)) if not lengths.empty else 0.0
            }
        }
    
    # Generate comprehensive statistics
    logging.info("\nðŸ“Š Calculating comprehensive statistics...")
    
    stats = {
        'timestamp': str(context.get('execution_date', datetime.now())),
        'validation_status': 'PASSED',
        'dataset_sizes': {
            'train': len(train_df),
            'test': len(test_df),
            'total': len(train_df) + len(test_df)
        },
        'schema': {
            'expected_columns': list(expected_schema.keys()),
            'expected_dtypes': expected_schema,
            'actual_train_dtypes': train_df[list(expected_schema.keys())].dtypes.astype(str).to_dict(),
            'actual_test_dtypes': test_df[list(expected_schema.keys())].dtypes.astype(str).to_dict()
        },
        'train_statistics': {
            'sql_prompt': calculate_text_statistics(train_df['sql_prompt'], 'sql_prompt'),
            'sql': calculate_text_statistics(train_df['sql'], 'sql'),
            'sql_context': calculate_text_statistics(train_df['sql_context'], 'sql_context'),
            'sql_complexity': {
                'distribution': train_df['sql_complexity'].value_counts().to_dict(),
                'unique_values': int(train_df['sql_complexity'].nunique()),
                'null_count': int(train_df['sql_complexity'].isnull().sum())
            }
        },
        'test_statistics': {
            'sql_prompt': calculate_text_statistics(test_df['sql_prompt'], 'sql_prompt'),
            'sql': calculate_text_statistics(test_df['sql'], 'sql'),
            'sql_context': calculate_text_statistics(test_df['sql_context'], 'sql_context'),
            'sql_complexity': {
                'distribution': test_df['sql_complexity'].value_counts().to_dict(),
                'unique_values': int(test_df['sql_complexity'].nunique())
            }
        },
        'data_quality_checks': {
            'train_duplicates': int(train_df.duplicated(subset=['sql_prompt', 'sql']).sum()),
            'test_duplicates': int(test_df.duplicated(subset=['sql_prompt', 'sql']).sum()),
            'train_empty_sql_prompt': int((train_df['sql_prompt'].str.strip() == '').sum()),
            'train_empty_sql': int((train_df['sql'].str.strip() == '').sum()),
            'test_empty_sql_prompt': int((test_df['sql_prompt'].str.strip() == '').sum()),
            'test_empty_sql': int((test_df['sql'].str.strip() == '').sum())
        }
    }
    
    # Save schema and statistics
    output_dir = '/opt/airflow/data'
    os.makedirs(output_dir, exist_ok=True)
    
    schema_path = f'{output_dir}/raw_schema_and_stats.json'
    with open(schema_path, 'w') as f:
        json.dump(stats, f, indent=2)
    
    logging.info(f"âœ… Raw schema and statistics saved to {schema_path}")
    
    # Log comprehensive statistics
    logging.info("\nðŸ“Š RAW DATA COMPREHENSIVE STATISTICS:")
    logging.info(f"   Dataset Sizes:")
    logging.info(f"      Train: {len(train_df):,} samples")
    logging.info(f"      Test: {len(test_df):,} samples")
    
    logging.info(f"\n   SQL Prompt Statistics (Train):")
    prompt_stats = stats['train_statistics']['sql_prompt']['length_stats']
    logging.info(f"      Min/Max/Mean: {prompt_stats['min']}/{prompt_stats['max']}/{prompt_stats['mean']:.1f} chars")
    logging.info(f"      Median: {prompt_stats['median']:.1f} chars")
    logging.info(f"      Std Dev: {prompt_stats['std']:.1f}")
    logging.info(f"      P50/P90/P99: {prompt_stats['median']:.0f}/{prompt_stats['percentile_90']:.0f}/{prompt_stats['percentile_99']:.0f}")
    
    logging.info(f"\n   SQL Query Statistics (Train):")
    sql_stats = stats['train_statistics']['sql']['length_stats']
    logging.info(f"      Min/Max/Mean: {sql_stats['min']}/{sql_stats['max']}/{sql_stats['mean']:.1f} chars")
    logging.info(f"      Median: {sql_stats['median']:.1f} chars")
    logging.info(f"      Std Dev: {sql_stats['std']:.1f}")
    logging.info(f"      P50/P90/P99: {sql_stats['median']:.0f}/{sql_stats['percentile_90']:.0f}/{sql_stats['percentile_99']:.0f}")
    
    logging.info(f"\n   SQL Context:")
    logging.info(f"      Null rate: {stats['train_statistics']['sql_context']['null_rate']:.2%}")
    
    logging.info(f"\n   SQL Complexity:")
    logging.info(f"      Unique classes: {stats['train_statistics']['sql_complexity']['unique_values']}")
    
    logging.info(f"\n   Data Quality:")
    logging.info(f"      Train duplicates: {stats['data_quality_checks']['train_duplicates']}")
    logging.info(f"      Test duplicates: {stats['data_quality_checks']['test_duplicates']}")
    logging.info(f"      Empty prompts/queries: {stats['data_quality_checks']['train_empty_sql_prompt']}/{stats['data_quality_checks']['train_empty_sql']}")
    
    # Warnings for data quality issues
    if stats['data_quality_checks']['train_duplicates'] > 0:
        logging.warning(f"âš ï¸ Found {stats['data_quality_checks']['train_duplicates']} duplicate samples in training data")
    
    # Push to XCom for later reference
    context['task_instance'].xcom_push(key='raw_schema_stats', value=stats)
    
    return {
        'schema_validated': True,
        'train_size': len(train_df),
        'test_size': len(test_df),
        'schema_path': schema_path,
        'validation_status': 'PASSED'
    }

def detect_bias(**context):
    """Detect class imbalance bias in the dataset and send email alert"""
    import pandas as pd
    from utils.EmailContentGenerator import generate_bias_detection_email, send_email_notification
    
    # Load preprocessed training data
    train_df = pd.read_pickle('/tmp/train_preprocessed.pkl')
    
    # Analyze sql_complexity distribution
    complexity_counts = train_df['sql_complexity'].value_counts()
    total_samples = len(train_df)
    
    logging.info(f"Analyzing bias in sql_complexity distribution...")
    logging.info(f"Total training samples: {total_samples}")
    logging.info(f"Complexity distribution:\n{complexity_counts}")
    
    # Calculate statistics
    max_count = complexity_counts.max()
    min_count = complexity_counts.min()
    max_class = complexity_counts.idxmax()
    min_class = complexity_counts.idxmin()
    
    # Calculate imbalance ratio
    imbalance_ratio = max_count / min_count if min_count > 0 else float('inf')
    
    # Calculate percentage distribution
    percentages = (complexity_counts / total_samples * 100).round(2)
    
    # Determine bias level
    bias_detected = False
    bias_level = "None"
    minority_classes = []
    
    # Define bias thresholds
    SEVERE_BIAS_RATIO = 10
    MODERATE_BIAS_RATIO = 5
    MINORITY_THRESHOLD = 0.5
    
    if imbalance_ratio >= SEVERE_BIAS_RATIO:
        bias_detected = True
        bias_level = "SEVERE"
    elif imbalance_ratio >= MODERATE_BIAS_RATIO:
        bias_detected = True
        bias_level = "MODERATE"
    elif imbalance_ratio >= 2:
        bias_detected = True
        bias_level = "MILD"
    
    # Identify minority classes
    for complexity, count in complexity_counts.items():
        if count < (max_count * MINORITY_THRESHOLD):
            minority_classes.append(f"{complexity} ({count} samples, {percentages[complexity]}%)")
    
    # Store bias info
    bias_info = {
        'bias_detected': bias_detected,
        'bias_level': bias_level,
        'imbalance_ratio': float(imbalance_ratio),
        'minority_classes': len(minority_classes),
        'max_class': max_class,
        'max_count': int(max_count),
        'min_class': min_class,
        'min_count': int(min_count)
    }
    
    # Generate and send email using utility function
    subject, html_content = generate_bias_detection_email(
        complexity_counts, total_samples, bias_info, minority_classes
    )
    send_email_notification(subject, html_content)
    
    # Push to XCom for downstream tasks
    context['task_instance'].xcom_push(key='bias_info', value=bias_info)
    
    logging.info(f"Bias detection complete: {bias_level} bias detected")
    return bias_info

def analyze_and_generate_synthetic():
    """Analyze class distribution and generate synthetic data"""
    import pandas as pd
    import sys
    import os
    sys.path.append('/opt/airflow/dags')
    from utils.DataGenerator import GenerateAdditionalData
    
    # Load preprocessed training data
    train_df = pd.read_pickle('/tmp/train_preprocessed.pkl')
    
    # Analyze sql_complexity distribution
    complexity_counts = train_df['sql_complexity'].value_counts()
    logging.info(f"Original complexity distribution:\n{complexity_counts}")
    
    # Find maximum count (target for balancing)
    max_count = complexity_counts.max()
    logging.info(f"Target count per complexity: {max_count}")
    
    # Calculate how many synthetic samples needed per complexity
    target_counts = {}
    minority_classes = ['CTEs', 'set operations', 'window functions', 
                       'subqueries', 'multiple_joins']
    
    for complexity in minority_classes:
        current_count = complexity_counts.get(complexity, 0)
        needed = max(0, max_count - current_count)
        if needed > 0:
            target_counts[complexity] = needed
            logging.info(f"{complexity}: need {needed} synthetic samples")
    
    if not target_counts:
        logging.info("Dataset already balanced, no synthetic data needed")
        synthetic_df = pd.DataFrame()
        synthetic_df.to_pickle('/tmp/synthetic_data.pkl')
        return {'synthetic_generated': 0}
    
    # Generate synthetic data
    logging.info(f"Generating synthetic data with target_counts: {target_counts}")
    GenerateAdditionalData(target_counts)
    
    # Load the generated CSV
    path = '/opt/airflow/data/synthetic_data.csv'
    
    if not os.path.exists(path):
        logging.error(f"Synthetic data file not found at {path}")
        synthetic_df = pd.DataFrame()
        synthetic_df.to_pickle('/tmp/synthetic_data.pkl')
        return {'synthetic_generated': 0}
    
    synthetic_df = pd.read_csv(path)
    logging.info(f"Loaded {len(synthetic_df)} synthetic samples from {path}")
    
    # Save as pickle for next task
    synthetic_df.to_pickle('/tmp/synthetic_data.pkl')
    
    logging.info(f"Generated {len(synthetic_df)} synthetic samples")
    return {'synthetic_generated': len(synthetic_df)}

def merge_and_split():
    """Merge original + synthetic for train, keep test separate"""
    import pandas as pd
    from sklearn.model_selection import train_test_split
    import os
    
    # Load data
    train_df = pd.read_pickle('/tmp/train_preprocessed.pkl')
    test_df = pd.read_pickle('/tmp/test_preprocessed.pkl')
    synthetic_df = pd.read_pickle('/tmp/synthetic_data.pkl')
    
    logging.info(f"Original train size: {len(train_df)}")
    logging.info(f"Synthetic data size: {len(synthetic_df)}")
    logging.info(f"Test size: {len(test_df)}")
    
    # Store original complexity distribution
    original_complexity_dist = train_df['sql_complexity'].value_counts().to_dict()
    
    # Merge original + synthetic for training
    if len(synthetic_df) > 0:
        train_cols = ['sql_prompt', 'sql_context', 'sql', 'sql_complexity']
        
        train_original = train_df[train_cols].copy()
        synthetic_data = synthetic_df[train_cols].copy()
        
        train_combined = pd.concat([train_original, synthetic_data], ignore_index=True)
        logging.info(f"Combined train size: {len(train_combined)}")
        
        final_dist = train_combined['sql_complexity'].value_counts()
        logging.info(f"Final complexity distribution:\n{final_dist}")
    else:
        train_combined = train_df[['sql_prompt', 'sql_context', 'sql', 'sql_complexity']].copy()
    
    # Store final complexity distribution
    final_complexity_dist = train_combined['sql_complexity'].value_counts().to_dict()
    
    # Create input_text column AFTER merging
    def format_input_text(row):
        prompt = "translate English to SQL: "
        if pd.notna(row['sql_context']):
            prompt += f"context: {row['sql_context']}\n\n"
        prompt += f"query: {row['sql_prompt']}"
        return prompt
    
    train_combined['input_text'] = train_combined.apply(format_input_text, axis=1)
    logging.info("Created input_text column from sql_context + sql_prompt")
    
    # Also create input_text for test data
    test_df['input_text'] = test_df.apply(format_input_text, axis=1)
    
    # Stratified split for train/val
    train_combined['strat'] = train_combined['sql_complexity'].astype(str)
    counts = train_combined['strat'].value_counts()
    valid_strat = counts[counts >= 2].index
    train_combined = train_combined[train_combined['strat'].isin(valid_strat)]
    
    train_final, val_final = train_test_split(
        train_combined, test_size=0.1, stratify=train_combined['strat'], random_state=42
    )
    
    # Drop stratification column
    train_final = train_final.drop(columns=['strat'])
    val_final = val_final.drop(columns=['strat'])
    
    # Save to /opt/airflow/data/ as CSV
    output_dir = '/opt/airflow/data'
    os.makedirs(output_dir, exist_ok=True)
    
    # Clean up old files
    for filename in ['train.csv', 'val.csv', 'test.csv']:
        old_file = f'{output_dir}/{filename}'
        if os.path.exists(old_file):
            try:
                os.remove(old_file)
                logging.info(f"Removed old file: {old_file}")
            except Exception as e:
                logging.warning(f"Could not remove {old_file}: {e}")
    
    # UPDATED: Save input_text, sql, AND sql_complexity columns
    save_cols = ['input_text', 'sql', 'sql_complexity']
    
    saved_files = []
    for name, df in [('train', train_final), ('val', val_final), ('test', test_df)]:
        path = f'{output_dir}/{name}.csv'
        df[save_cols].to_csv(path, index=False)
        saved_files.append(path)
        logging.info(f"Saved {len(df)} rows to {path}")
    
    return {
        'files': saved_files,
        'train': len(train_final),
        'val': len(val_final),
        'test': len(test_df),
        'train_original': len(train_df),
        'synthetic_added': len(synthetic_df),
        'original_complexity_dist': original_complexity_dist,
        'final_complexity_dist': final_complexity_dist
    }

def remove_data_leakage(**context):
    """Remove duplicate data within and between train/val/test splits to prevent data leakage"""
    import pandas as pd
    import os
    
    logging.info("=" * 60)
    logging.info("REMOVING DUPLICATES AND DATA LEAKAGE FROM SPLITS")
    logging.info("=" * 60)
    
    # Load final datasets
    output_dir = '/opt/airflow/data'
    train_df = pd.read_csv(f'{output_dir}/train.csv')
    val_df = pd.read_csv(f'{output_dir}/val.csv')
    test_df = pd.read_csv(f'{output_dir}/test.csv')
    
    original_train_size = len(train_df)
    original_val_size = len(val_df)
    original_test_size = len(test_df)
    
    logging.info(f"Original sizes - Train: {len(train_df):,}, Val: {len(val_df):,}, Test: {len(test_df):,}")
    
    # ============================================================================
    # STEP 1: Remove intra-split duplicates (within each split)
    # ============================================================================
    logging.info("\nðŸ” STEP 1: Removing intra-split duplicates...")
    
    # Remove duplicates within train
    train_duplicates_before = train_df.duplicated(subset=['input_text', 'sql']).sum()
    if train_duplicates_before > 0:
        logging.warning(f"   âš ï¸ Found {train_duplicates_before} duplicate samples within train set")
        train_df = train_df.drop_duplicates(subset=['input_text', 'sql'], keep='first')
        logging.info(f"   âœ… Removed {train_duplicates_before} duplicates from train")
    else:
        logging.info(f"   âœ… No duplicates found within train set")
    
    # Remove duplicates within val
    val_duplicates_before = val_df.duplicated(subset=['input_text', 'sql']).sum()
    if val_duplicates_before > 0:
        logging.warning(f"   âš ï¸ Found {val_duplicates_before} duplicate samples within val set")
        val_df = val_df.drop_duplicates(subset=['input_text', 'sql'], keep='first')
        logging.info(f"   âœ… Removed {val_duplicates_before} duplicates from val")
    else:
        logging.info(f"   âœ… No duplicates found within val set")
    
    # Remove duplicates within test
    test_duplicates_before = test_df.duplicated(subset=['input_text', 'sql']).sum()
    if test_duplicates_before > 0:
        logging.warning(f"   âš ï¸ Found {test_duplicates_before} duplicate samples within test set")
        test_df = test_df.drop_duplicates(subset=['input_text', 'sql'], keep='first')
        logging.info(f"   âœ… Removed {test_duplicates_before} duplicates from test")
    else:
        logging.info(f"   âœ… No duplicates found within test set")
    
    after_intra_removal_train = len(train_df)
    after_intra_removal_val = len(val_df)
    after_intra_removal_test = len(test_df)
    
    logging.info(f"\n   After intra-split deduplication:")
    logging.info(f"      Train: {original_train_size:,} â†’ {after_intra_removal_train:,} (-{train_duplicates_before})")
    logging.info(f"      Val: {original_val_size:,} â†’ {after_intra_removal_val:,} (-{val_duplicates_before})")
    logging.info(f"      Test: {original_test_size:,} â†’ {after_intra_removal_test:,} (-{test_duplicates_before})")
    
    # ============================================================================
    # STEP 2: Remove cross-split duplicates (between splits)
    # ============================================================================
    logging.info("\nðŸ” STEP 2: Checking for cross-split data leakage...")
    
    # Create combined content for each split
    train_df['combined_content'] = train_df['input_text'] + '|||' + train_df['sql']
    val_df['combined_content'] = val_df['input_text'] + '|||' + val_df['sql']
    test_df['combined_content'] = test_df['input_text'] + '|||' + test_df['sql']
    
    train_content_set = set(train_df['combined_content'].values)
    val_content_set = set(val_df['combined_content'].values)
    test_content_set = set(test_df['combined_content'].values)
    
    # Check for overlaps
    train_val_overlap = train_content_set & val_content_set
    train_test_overlap = train_content_set & test_content_set
    val_test_overlap = val_content_set & test_content_set
    
    # Log results
    logging.info(f"Checking for data leakage...")
    logging.info(f"   Train samples: {len(train_content_set):,}")
    logging.info(f"   Val samples: {len(val_content_set):,}")
    logging.info(f"   Test samples: {len(test_content_set):,}")
    
    # Track if we removed any samples from train due to cross-split overlap
    removed_from_train_cross_split = 0
    leakage_report = []
    
    # Handle train-val overlap: remove from train
    if train_val_overlap:
        logging.warning(f"   âš ï¸ Found {len(train_val_overlap)} overlapping samples between train and val")
        
        # Log first few examples
        for i, sample in enumerate(list(train_val_overlap)[:3]):
            logging.warning(f"      Example {i+1}: {sample[:100]}...")
        
        # Remove from train
        train_before = len(train_df)
        train_df = train_df[~train_df['combined_content'].isin(train_val_overlap)]
        train_after = len(train_df)
        removed_count = train_before - train_after
        removed_from_train_cross_split += removed_count
        
        logging.info(f"   âœ… Removed {removed_count} duplicate samples from train set")
        leakage_report.append(f"Removed {removed_count} train samples that appeared in val")
    
    # Handle train-test overlap: remove from train
    if train_test_overlap:
        logging.warning(f"   âš ï¸ Found {len(train_test_overlap)} overlapping samples between train and test")
        
        # Log first few examples
        for i, sample in enumerate(list(train_test_overlap)[:3]):
            logging.warning(f"      Example {i+1}: {sample[:100]}...")
        
        # Remove from train
        train_before = len(train_df)
        train_df = train_df[~train_df['combined_content'].isin(train_test_overlap)]
        train_after = len(train_df)
        removed_count = train_before - train_after
        removed_from_train_cross_split += removed_count
        
        logging.info(f"   âœ… Removed {removed_count} duplicate samples from train set")
        leakage_report.append(f"Removed {removed_count} train samples that appeared in test")
    
    # Handle val-test overlap: this is a critical error that we cannot auto-fix
    if val_test_overlap:
        error_msg = f"CRITICAL: {len(val_test_overlap)} identical samples appear in both val and test - cannot auto-fix"
        logging.error(f"   âŒ {error_msg}")
        
        # Log first few examples
        for i, sample in enumerate(list(val_test_overlap)[:3]):
            logging.error(f"      Example {i+1}: {sample[:100]}...")
        
        raise ValueError(error_msg)
    
    # Log cross-split leakage results
    if removed_from_train_cross_split > 0 or train_val_overlap or train_test_overlap:
        logging.info(f"\n   Cross-split leakage summary:")
        logging.info(f"      Train-Val overlap: {len(train_val_overlap)}")
        logging.info(f"      Train-Test overlap: {len(train_test_overlap)}")
        logging.info(f"      Total removed from train: {removed_from_train_cross_split}")
    else:
        logging.info(f"   âœ… No cross-split data leakage detected")
    
    # ============================================================================
    # STEP 3: Save cleaned datasets
    # ============================================================================
    final_train_size = len(train_df)
    final_val_size = len(val_df)
    final_test_size = len(test_df)
    
    total_train_removed = original_train_size - final_train_size
    total_val_removed = original_val_size - final_val_size
    total_test_removed = original_test_size - final_test_size
    
    if total_train_removed > 0 or total_val_removed > 0 or total_test_removed > 0:
        logging.info(f"\nðŸ”§ Saving cleaned datasets...")
        logging.info(f"   Train: {original_train_size:,} â†’ {final_train_size:,} (removed {total_train_removed})")
        logging.info(f"   Val: {original_val_size:,} â†’ {final_val_size:,} (removed {total_val_removed})")
        logging.info(f"   Test: {original_test_size:,} â†’ {final_test_size:,} (removed {total_test_removed})")
        
        # Clean up temporary columns and save - PRESERVE sql_complexity
        train_df_clean = train_df.drop(columns=['combined_content'])
        val_df_clean = val_df.drop(columns=['combined_content'])
        test_df_clean = test_df.drop(columns=['combined_content'])
        
        train_path = f'{output_dir}/train.csv'
        val_path = f'{output_dir}/val.csv'
        test_path = f'{output_dir}/test.csv'
        
        train_df_clean.to_csv(train_path, index=False)
        val_df_clean.to_csv(val_path, index=False)
        test_df_clean.to_csv(test_path, index=False)
        
        logging.info(f"   âœ… Saved cleaned datasets")
    else:
        logging.info("âœ… No duplicates found - datasets already clean")
        # Still clean up temporary columns
        train_df = train_df.drop(columns=['combined_content'], errors='ignore')
        val_df = val_df.drop(columns=['combined_content'], errors='ignore')
        test_df = test_df.drop(columns=['combined_content'], errors='ignore')
    
    # Push comprehensive stats to XCom
    leakage_stats = {
        'original_train_size': original_train_size,
        'original_val_size': original_val_size,
        'original_test_size': original_test_size,
        'final_train_size': final_train_size,
        'final_val_size': final_val_size,
        'final_test_size': final_test_size,
        'intra_split_duplicates': {
            'train_duplicates_removed': train_duplicates_before,
            'val_duplicates_removed': val_duplicates_before,
            'test_duplicates_removed': test_duplicates_before,
            'total_intra_removed': train_duplicates_before + val_duplicates_before + test_duplicates_before
        },
        'cross_split_leakage': {
            'removed_from_train': removed_from_train_cross_split,
            'train_val_overlap': len(train_val_overlap),
            'train_test_overlap': len(train_test_overlap),
            'val_test_overlap': len(val_test_overlap),
            'leakage_report': leakage_report
        },
        'total_removed': {
            'train': total_train_removed,
            'val': total_val_removed,
            'test': total_test_removed
        },
        'leakage_cleaned': (removed_from_train_cross_split > 0 or 
                           train_duplicates_before > 0 or 
                           val_duplicates_before > 0 or 
                           test_duplicates_before > 0)
    }
    
    context['task_instance'].xcom_push(key='leakage_stats', value=leakage_stats)
    
    logging.info("\n" + "=" * 60)
    logging.info("DUPLICATE REMOVAL SUMMARY")
    logging.info("=" * 60)
    logging.info(f"Intra-split duplicates removed: {leakage_stats['intra_split_duplicates']['total_intra_removed']}")
    logging.info(f"Cross-split duplicates removed: {removed_from_train_cross_split}")
    logging.info(f"Total samples removed: {total_train_removed + total_val_removed + total_test_removed}")
    logging.info("=" * 60)
    
    return {
        'original_train_size': original_train_size,
        'final_train_size': final_train_size,
        'train_intra_duplicates_removed': train_duplicates_before,
        'train_cross_duplicates_removed': removed_from_train_cross_split,
        'total_train_removed': total_train_removed,
        'val_duplicates_removed': total_val_removed,
        'test_duplicates_removed': total_test_removed,
        'leakage_cleaned': leakage_stats['leakage_cleaned']
    }

def final_data_validation_check(**context):
    """Clean final datasets by removing INSERT INTO statements and unnecessary phrases"""
    import pandas as pd
    import re
    import os
    
    logging.info("=" * 60)
    logging.info("CLEANING FINAL DATASETS")
    logging.info("=" * 60)
    
    output_dir = '/opt/airflow/data'
    
    def remove_context(x: str) -> str:
        return re.sub(r'(?i)insert\s+into.*?\n\nquery:', 'query:', x, flags=re.DOTALL)
    
    def translate(x: str) -> str:
        return re.sub(r'(?i)translate\s+english\s+to\s+sql.*?context:', 'context:', x, flags=re.DOTALL)
    
    stats = {}
    
    for filename in ['train.csv', 'val.csv', 'test.csv']:
        filepath = f'{output_dir}/{filename}'
        df = pd.read_csv(filepath)
        
        original_size = len(df)
        logging.info(f"\n Processing {filename}:")
        logging.info(f"   Original size: {original_size:,}")
        
        df['input_text'] = df['input_text'].apply(remove_context)
        df['input_text'] = df['input_text'].apply(translate)
        
        insert_rows = df['sql'].str.contains(r'insert\s+into', case=False, na=False).sum()
        df = df[~df['sql'].str.contains(r'insert\s+into', case=False, na=False)]
        
        df['sql'] = df['sql'].str.replace(r"\n", " ", regex=True)
        
        final_size = len(df)
        removed = original_size - final_size
        
        df.to_csv(filepath, index=False)
        
        logging.info(f"   Removed INSERT INTO rows: {insert_rows}")
        logging.info(f"   Final size: {final_size:,}")
        logging.info(f"   âœ… Cleaned and saved {filename}")
        
        stats[filename] = {
            'original': original_size,
            'final': final_size,
            'removed': removed
        }
    
    context['task_instance'].xcom_push(key='cleaning_stats', value=stats)
    
    logging.info("\n" + "=" * 60)
    logging.info("DATA CLEANING COMPLETE")
    logging.info("=" * 60)
    
    return stats

def validate_engineered_schema(**context):
    """Validate engineered features schema with comprehensive data quality checks - MEMORY OPTIMIZED"""
    import pandas as pd
    import numpy as np
    import json
    import os
    import gc
    
    logging.info("=" * 60)
    logging.info("VALIDATING ENGINEERED FEATURES SCHEMA")
    logging.info("=" * 60)
    
    # Load final datasets
    output_dir = '/opt/airflow/data'
    train_df = pd.read_csv(f'{output_dir}/train.csv')
    val_df = pd.read_csv(f'{output_dir}/val.csv')
    test_df = pd.read_csv(f'{output_dir}/test.csv')
    
    # UPDATED: Include sql_complexity in expected schema
    expected_schema = {
        'input_text': 'object',
        'sql': 'object',
        'sql_complexity': 'object'
    }
    
    validation_errors = []
    
    # 1. Validate columns and data types for all splits
    logging.info("\nðŸ” Validating Schema and Data Types...")
    
    for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:
        cols = df.columns.tolist()
        
        # Check columns match exactly
        if cols != list(expected_schema.keys()):
            validation_errors.append(f"{name} has wrong columns: expected {list(expected_schema.keys())}, got {cols}")
            logging.error(f"âŒ {name} columns mismatch")
        else:
            logging.info(f"âœ… {name} columns correct: {cols}")
        
        # Check data types
        for col, expected_dtype in expected_schema.items():
            if col in df.columns:
                actual_dtype = str(df[col].dtype)
                if actual_dtype != expected_dtype:
                    validation_errors.append(
                        f"{name}.{col} wrong dtype: expected {expected_dtype}, got {actual_dtype}"
                    )
                    logging.warning(f"âš ï¸ {name}.{col}: expected {expected_dtype}, got {actual_dtype}")
                else:
                    logging.info(f"âœ… {name}.{col}: {actual_dtype}")
        
        # Check for null values in critical columns
        critical_cols = ['input_text', 'sql', 'sql_complexity']
        null_counts = df[critical_cols].isnull().sum()
        if null_counts.any():
            validation_errors.append(f"{name} has null values: {null_counts.to_dict()}")
            logging.error(f"âŒ {name} contains null values: {null_counts.to_dict()}")
        
        # Check for empty strings
        for col in ['input_text', 'sql']:
            empty_count = (df[col].str.strip() == '').sum()
            if empty_count > 0:
                validation_errors.append(f"{name}.{col} has {empty_count} empty strings")
                logging.error(f"âŒ {name}.{col}: {empty_count} empty strings")
    
    # 2. Validate input_text format
    logging.info("\nðŸ” Validating input_text Format...")
    
    for name, df in [('train', train_df), ('val', val_df), ('test', test_df)]:
        
        # Must have "query:" keyword
        has_query = df['input_text'].str.contains('query:', na=False, regex=False).all()
        if not has_query:
            validation_errors.append(f"{name} input_text missing 'query:' keyword")
            logging.error(f"âŒ {name} input_text missing query section")
        
        if has_query:
            logging.info(f"âœ… {name} input_text format validated")
    
    # Raise error if any validation failed
    if validation_errors:
        error_msg = "\n".join(validation_errors)
        logging.error(f"\nâŒ SCHEMA VALIDATION FAILED:\n{error_msg}")
        raise ValueError(f"Schema validation failed:\n{error_msg}")
    
    logging.info("\nâœ… All schema validations passed")
    
    # MEMORY OPTIMIZED: Helper function for efficient text statistics
    def calculate_text_stats_efficient(series, column_name):
        """Calculate essential statistics efficiently without storing all intermediate results"""
        # Calculate lengths once
        lengths = series.str.len()
        
        # Calculate only essential percentiles
        percentiles = lengths.quantile([0.5, 0.9, 0.99]).to_dict()
        
        stats = {
            'count': int(series.notna().sum()),
            'null_count': int(series.isnull().sum()),
            'unique_count': int(series.nunique()),
            'duplicate_count': int(series.duplicated().sum()),
            'length_stats': {
                'min': int(lengths.min()),
                'max': int(lengths.max()),
                'mean': float(lengths.mean()),
                'median': float(percentiles[0.5]),
                'std': float(lengths.std()),
                'percentile_90': float(percentiles[0.9]),
                'percentile_99': float(percentiles[0.99])
            }
        }
        
        # Free memory
        del lengths, percentiles
        gc.collect()
        
        return stats
    
    # Helper function to convert pandas/numpy types to native Python types
    def convert_to_native_types(obj):
        """Recursively convert numpy/pandas types to native Python types for JSON serialization"""
        if isinstance(obj, dict):
            return {key: convert_to_native_types(val) for key, val in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [convert_to_native_types(item) for item in obj]
        elif isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif pd.isna(obj):
            return None
        else:
            return obj
    
    # Get leakage stats from previous task
    leakage_stats = context['task_instance'].xcom_pull(
        task_ids='remove_data_leakage',
        key='leakage_stats'
    ) or {}
    
    # Generate MEMORY-EFFICIENT comprehensive statistics
    logging.info("\nðŸ“Š Calculating statistics (memory-optimized)...")
    
    stats = {
        'timestamp': str(context.get('execution_date', datetime.now())),
        'validation_status': 'PASSED',
        'schema': {
            'expected_columns': list(expected_schema.keys()),
            'expected_dtypes': expected_schema,
            'actual_dtypes': train_df.dtypes.astype(str).to_dict()
        },
        'dataset_sizes': {
            'train': len(train_df),
            'val': len(val_df),
            'test': len(test_df),
            'total': len(train_df) + len(val_df) + len(test_df),
            'train_val_ratio': float(len(train_df) / len(val_df)) if len(val_df) > 0 else 0.0
        }
    }
    
    # Calculate train statistics
    logging.info("   Calculating train statistics...")
    train_complexity_dist = train_df['sql_complexity'].value_counts().to_dict()
    stats['train_statistics'] = {
        'input_text': calculate_text_stats_efficient(train_df['input_text'], 'input_text'),
        'sql': calculate_text_stats_efficient(train_df['sql'], 'sql'),
        'sql_complexity': {
            'distribution': {str(k): int(v) for k, v in train_complexity_dist.items()},
            'unique_values': int(train_df['sql_complexity'].nunique()),
            'null_count': int(train_df['sql_complexity'].isnull().sum())
        }
    }
    gc.collect()
    
    # Calculate val statistics
    logging.info("   Calculating val statistics...")
    val_complexity_dist = val_df['sql_complexity'].value_counts().to_dict()
    stats['val_statistics'] = {
        'input_text': calculate_text_stats_efficient(val_df['input_text'], 'input_text'),
        'sql': calculate_text_stats_efficient(val_df['sql'], 'sql'),
        'sql_complexity': {
            'distribution': {str(k): int(v) for k, v in val_complexity_dist.items()},
            'unique_values': int(val_df['sql_complexity'].nunique()),
            'null_count': int(val_df['sql_complexity'].isnull().sum())
        }
    }
    gc.collect()
    
    # Calculate test statistics
    logging.info("   Calculating test statistics...")
    test_complexity_dist = test_df['sql_complexity'].value_counts().to_dict()
    stats['test_statistics'] = {
        'input_text': calculate_text_stats_efficient(test_df['input_text'], 'input_text'),
        'sql': calculate_text_stats_efficient(test_df['sql'], 'sql'),
        'sql_complexity': {
            'distribution': {str(k): int(v) for k, v in test_complexity_dist.items()},
            'unique_values': int(test_df['sql_complexity'].nunique()),
            'null_count': int(test_df['sql_complexity'].isnull().sum())
        }
    }
    gc.collect()
    
    # Add feature engineering metrics (lightweight)
    stats['feature_engineering_metrics'] = {
        'input_text': {
            'train_has_context_rate': float(train_df['input_text'].str.contains('context:', na=False, regex=False).mean()),
            'val_has_context_rate': float(val_df['input_text'].str.contains('context:', na=False, regex=False).mean()),
            'test_has_context_rate': float(test_df['input_text'].str.contains('context:', na=False, regex=False).mean())
        }
    }
    
    # Add data quality checks
    stats['data_quality_checks'] = {
        'train_duplicates': int(train_df.duplicated(subset=['input_text', 'sql']).sum()),
        'val_duplicates': int(val_df.duplicated(subset=['input_text', 'sql']).sum()),
        'test_duplicates': int(test_df.duplicated(subset=['input_text', 'sql']).sum()),
        'data_leakage': {
            'method': 'concatenated_input_text_and_sql',
            'original_sizes': {
                'train': leakage_stats.get('original_train_size', len(train_df)),
                'val': leakage_stats.get('original_val_size', len(val_df)),
                'test': leakage_stats.get('original_test_size', len(test_df))
            },
            'final_sizes': {
                'train': leakage_stats.get('final_train_size', len(train_df)),
                'val': leakage_stats.get('final_val_size', len(val_df)),
                'test': leakage_stats.get('final_test_size', len(test_df))
            },
            'intra_split_duplicates': leakage_stats.get('intra_split_duplicates', {}),
            'cross_split_leakage': leakage_stats.get('cross_split_leakage', {}),
            'total_removed': leakage_stats.get('total_removed', {}),
            'leakage_cleaned': leakage_stats.get('leakage_cleaned', False)
        }
    }
    
    # Save engineered schema and statistics (with type conversion)
    schema_path = f'{output_dir}/engineered_schema_and_stats.json'
    
    # Convert all numpy/pandas types to native Python types before saving
    stats_serializable = convert_to_native_types(stats)
    
    with open(schema_path, 'w') as f:
        json.dump(stats_serializable, f, indent=2)
    
    logging.info(f"âœ… Engineered schema and statistics saved to {schema_path}")
    
    # Log comprehensive statistics
    logging.info("\nðŸ“Š ENGINEERED FEATURES COMPREHENSIVE STATISTICS:")
    logging.info(f"\n   Dataset Sizes:")
    logging.info(f"      Train: {len(train_df):,} samples")
    logging.info(f"      Validation: {len(val_df):,} samples")
    logging.info(f"      Test: {len(test_df):,} samples")
    logging.info(f"      Train/Val ratio: {stats['dataset_sizes']['train_val_ratio']:.1f}:1")
    
    logging.info(f"\n   Input Text Statistics (Train):")
    input_stats = stats['train_statistics']['input_text']
    logging.info(f"      Char length - Min/Max/Mean: {input_stats['length_stats']['min']}/{input_stats['length_stats']['max']}/{input_stats['length_stats']['mean']:.1f}")
    logging.info(f"      Char length - Median/P90/P99: {input_stats['length_stats']['median']:.0f}/{input_stats['length_stats']['percentile_90']:.0f}/{input_stats['length_stats']['percentile_99']:.0f}")
    logging.info(f"      Has context rate: {stats['feature_engineering_metrics']['input_text']['train_has_context_rate']:.2%}")
    logging.info(f"      Duplicates: {input_stats['duplicate_count']}")
    
    logging.info(f"\n   SQL Statistics (Train):")
    sql_stats = stats['train_statistics']['sql']
    logging.info(f"      Char length - Min/Max/Mean: {sql_stats['length_stats']['min']}/{sql_stats['length_stats']['max']}/{sql_stats['length_stats']['mean']:.1f}")
    logging.info(f"      Char length - Median/P90/P99: {sql_stats['length_stats']['median']:.0f}/{sql_stats['length_stats']['percentile_90']:.0f}/{sql_stats['length_stats']['percentile_99']:.0f}")
    logging.info(f"      Duplicates: {sql_stats['duplicate_count']}")
    
    # Log SQL complexity distribution
    logging.info(f"\n   SQL Complexity Distribution (Train):")
    train_size = stats['dataset_sizes']['train']  # Use stored size
    for complexity, count in sorted(train_complexity_dist.items(), key=lambda x: x[1], reverse=True):
        percentage = (count / train_size) * 100
        logging.info(f"      {complexity}: {count:,} ({percentage:.1f}%)")
    
    logging.info(f"\n   Data Quality:")
    logging.info(f"      Duplicate removal method: 2-step (intra-split + cross-split)")
    
    intra_dups = leakage_stats.get('intra_split_duplicates', {})
    cross_leakage = leakage_stats.get('cross_split_leakage', {})
    total_removed = leakage_stats.get('total_removed', {})
    
    # Log summary
    total_intra_removed = intra_dups.get('total_intra_removed', 0)
    cross_removed = cross_leakage.get('removed_from_train', 0)
    
    if total_intra_removed > 0:
        logging.info(f"      âœ… Intra-split duplicates removed: {total_intra_removed}")
    
    if cross_removed > 0:
        logging.info(f"      âš ï¸ Cross-split leakage cleaned: {cross_removed} from train")
    
    if total_intra_removed == 0 and cross_removed == 0:
        logging.info(f"      âœ… No duplicates or leakage detected")
    
    # Free memory
    del train_df, val_df, test_df
    gc.collect()
    
    return {
        'schema_validated': True,
        'train_size': stats['dataset_sizes']['train'],
        'val_size': stats['dataset_sizes']['val'],
        'test_size': stats['dataset_sizes']['test'],
        'schema_path': schema_path,
        'validation_status': 'PASSED',
        'leakage_cleaned': leakage_stats.get('leakage_cleaned', False),
        'intra_duplicates_removed': total_intra_removed,
        'cross_duplicates_removed': cross_removed,
        'total_duplicates_removed': sum(total_removed.values()) if total_removed else 0
    }

def send_pipeline_success_notification(**context):
    """Send success notification email with pipeline statistics"""
    import pandas as pd
    from utils.EmailContentGenerator import send_email_notification
    
    # Get results from previous tasks via XCom
    task_instance = context['task_instance']
    
    # Get stats from different tasks
    load_stats = task_instance.xcom_pull(task_ids='load_data') or {}
    validate_stats = task_instance.xcom_pull(task_ids='validate_sql') or {}
    raw_schema_stats = task_instance.xcom_pull(task_ids='validate_raw_schema') or {}
    bias_info = task_instance.xcom_pull(task_ids='detect_bias', key='bias_info') or {}
    synthetic_stats = task_instance.xcom_pull(task_ids='analyze_and_generate_synthetic') or {}
    merge_stats = task_instance.xcom_pull(task_ids='merge_and_split') or {}
    leakage_stats = task_instance.xcom_pull(task_ids='remove_data_leakage', key='leakage_stats') or {}
    engineered_schema_stats = task_instance.xcom_pull(task_ids='validate_engineered_schema') or {}
    
    # Get FINAL sizes after all processing (including deduplication)
    final_train_size = engineered_schema_stats.get('train_size', merge_stats.get('train', 0))
    final_val_size = engineered_schema_stats.get('val_size', merge_stats.get('val', 0))
    final_test_size = engineered_schema_stats.get('test_size', merge_stats.get('test', 0))
    
    # Get duplicate removal stats
    total_duplicates_removed = engineered_schema_stats.get('total_duplicates_removed', 0)
    intra_duplicates_removed = engineered_schema_stats.get('intra_duplicates_removed', 0)
    cross_duplicates_removed = engineered_schema_stats.get('cross_duplicates_removed', 0)
    
    # Calculate improvement metrics
    original_dist = merge_stats.get('original_complexity_dist', {})
    final_dist = merge_stats.get('final_complexity_dist', {})
    
    # Get final SQL complexity distribution from engineered schema stats
    try:
        final_complexity_distribution = engineered_schema_stats.get('train_statistics', {}).get('sql_complexity', {}).get('distribution', {})
    except:
        final_complexity_distribution = {}
    
    # Build success email
    subject = "âœ… Pipeline Success - Data Processing Complete with Schema Validation"
    
    html_content = f"""
    <html>
    <head>
        <style>
            body {{ font-family: Arial, sans-serif; }}
            .header {{ background-color: #27ae60; color: white; padding: 20px; }}
            .content {{ padding: 20px; }}
            .stats-box {{ background-color: #f8f9fa; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #27ae60; }}
            .warning-box {{ background-color: #fff3cd; padding: 15px; margin: 10px 0; border-radius: 5px; border-left: 4px solid #ffc107; }}
            .success {{ color: #27ae60; font-weight: bold; }}
            .info {{ color: #0984e3; }}
            table {{ border-collapse: collapse; width: 100%; margin: 15px 0; }}
            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}
            th {{ background-color: #27ae60; color: white; }}
            .improved {{ background-color: #d4edda; }}
            .metric {{ font-size: 24px; font-weight: bold; color: #27ae60; }}
            .schema-badge {{ background-color: #27ae60; color: white; padding: 5px 10px; border-radius: 3px; font-size: 12px; }}
            .progress-bar {{ background-color: #e9ecef; border-radius: 5px; height: 20px; margin: 5px 0; }}
            .progress-fill {{ background-color: #27ae60; height: 100%; border-radius: 5px; text-align: center; color: white; font-size: 12px; line-height: 20px; }}
        </style>
    </head>
    <body>
        <div class="header">
            <h2>âœ… QueryHub Data Pipeline - Success Report</h2>
            <p>Text-to-SQL Dataset Processing Complete with Comprehensive Schema Validation</p>
        </div>
        
        <div class="content">
            <h3 class="success">ðŸŽ‰ Pipeline Execution Successful!</h3>
            <p>Your data pipeline has completed successfully with bias mitigation, data leakage prevention, and comprehensive schema validation applied.</p>
            
            <div class="stats-box">
                <h4>ðŸ“Š Pipeline Summary</h4>
                <table>
                    <tr>
                        <th>Stage</th>
                        <th>Metric</th>
                        <th>Value</th>
                    </tr>
                    <tr>
                        <td><strong>Data Loading</strong></td>
                        <td>Initial Training Samples</td>
                        <td>{load_stats.get('train', 0):,}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Initial Test Samples</td>
                        <td>{load_stats.get('test', 0):,}</td>
                    </tr>
                    <tr>
                        <td><strong>SQL Validation</strong></td>
                        <td>Valid Training Queries</td>
                        <td>{validate_stats.get('train_valid', 0):,}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Valid Test Queries</td>
                        <td>{validate_stats.get('test_valid', 0):,}</td>
                    </tr>
                    <tr class="warning-box">
                        <td></td>
                        <td>âš ï¸ Anomalies Detected</td>
                        <td>{validate_stats.get('total_anomalies', 0):,}</td>
                    </tr>
                    <tr class="improved">
                        <td><strong>Raw Schema Validation</strong></td>
                        <td>Status <span class="schema-badge">COMPREHENSIVE</span></td>
                        <td>âœ… {raw_schema_stats.get('validation_status', 'PASSED')}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Data Types Validated</td>
                        <td>âœ… All string columns verified</td>
                    </tr>
                    <tr>
                        <td><strong>Bias Detection</strong></td>
                        <td>Bias Level</td>
                        <td>{bias_info.get('bias_level', 'Unknown')}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Imbalance Ratio</td>
                        <td>{bias_info.get('imbalance_ratio', 0):.2f}x</td>
                    </tr>
                    <tr class="improved">
                        <td><strong>Synthetic Generation</strong></td>
                        <td>Synthetic Samples Added</td>
                        <td class="metric">{synthetic_stats.get('synthetic_generated', 0):,}</td>
                    </tr>
    """
    
    # Add duplicate removal section if any duplicates were removed
    if total_duplicates_removed > 0:
        html_content += f"""
                    <tr class="improved">
                        <td><strong>Duplicate Removal</strong></td>
                        <td>Total Duplicates Removed</td>
                        <td class="metric">{total_duplicates_removed:,}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Intra-split Duplicates</td>
                        <td>{intra_duplicates_removed:,}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Cross-split Leakage</td>
                        <td>{cross_duplicates_removed:,}</td>
                    </tr>
        """
    
    html_content += f"""
                    <tr class="improved">
                        <td><strong>Engineered Schema Validation</strong></td>
                        <td>Status <span class="schema-badge">COMPREHENSIVE</span></td>
                        <td>âœ… {engineered_schema_stats.get('validation_status', 'PASSED')}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Data Leakage Check</td>
                        <td>âœ… No leakage detected</td>
                    </tr>
                    <tr class="improved">
                        <td><strong>Final Dataset (After All Processing)</strong></td>
                        <td>Training Samples</td>
                        <td class="metric">{final_train_size:,}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Validation Samples</td>
                        <td>{final_val_size:,}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td>Test Samples</td>
                        <td>{final_test_size:,}</td>
                    </tr>
                    <tr>
                        <td></td>
                        <td><strong>Total Samples</strong></td>
                        <td><strong>{final_train_size + final_val_size + final_test_size:,}</strong></td>
                    </tr>
                </table>
            </div>
    """
    
    # Add anomalies section if any were found
    if validate_stats.get('total_anomalies', 0) > 0:
        html_content += f"""
            <div class="warning-box">
                <h4>âš ï¸ SQL Validation Anomalies</h4>
                <p>Detected <strong>{validate_stats.get('total_anomalies', 0)}</strong> invalid SQL queries that were excluded from the dataset:</p>
                <ul>
                    <li>Training set anomalies: {validate_stats.get('train_anomalies', 0)}</li>
                    <li>Test set anomalies: {validate_stats.get('test_anomalies', 0)}</li>
                </ul>
                <p>ðŸ“„ Full anomaly report saved to: <code>/opt/airflow/data/sql_validation_anomalies.csv</code></p>
            </div>
        """
    
    # Add final SQL complexity distribution (NEW SECTION)
    if final_complexity_distribution:
        html_content += """
            <div class="stats-box">
                <h4>ðŸŽ¯ Final Training Dataset - SQL Complexity Distribution</h4>
                <p><em>After all processing: bias mitigation, synthetic generation, and duplicate removal</em></p>
                <table>
                    <tr>
                        <th>SQL Complexity</th>
                        <th>Count</th>
                        <th>Percentage</th>
                        <th>Distribution</th>
                    </tr>
        """
        
        # Calculate total for percentages
        total_samples = sum(final_complexity_distribution.values())
        
        # Sort by count descending
        sorted_complexity = sorted(final_complexity_distribution.items(), key=lambda x: x[1], reverse=True)
        
        for complexity, count in sorted_complexity:
            percentage = (count / total_samples * 100) if total_samples > 0 else 0
            bar_width = percentage
            
            html_content += f"""
                    <tr>
                        <td><strong>{complexity}</strong></td>
                        <td>{count:,}</td>
                        <td>{percentage:.2f}%</td>
                        <td>
                            <div class="progress-bar">
                                <div class="progress-fill" style="width: {bar_width}%">{percentage:.1f}%</div>
                            </div>
                        </td>
                    </tr>
            """
        
        html_content += f"""
                    <tr style="background-color: #e8f5e9; font-weight: bold;">
                        <td><strong>TOTAL</strong></td>
                        <td><strong>{total_samples:,}</strong></td>
                        <td><strong>100.00%</strong></td>
                        <td></td>
                    </tr>
                </table>
            </div>
        """
    
    # Add class distribution comparison (Before vs After bias mitigation)
    html_content += """
            <div class="stats-box">
                <h4>ðŸ“ˆ Class Distribution - Before vs After Bias Mitigation</h4>
                <p><em>Comparison of original dataset vs after synthetic data generation (before deduplication)</em></p>
                <table>
                    <tr>
                        <th>SQL Complexity</th>
                        <th>Original Count</th>
                        <th>After Synthetic</th>
                        <th>Change</th>
                    </tr>
    """
    
    all_classes = set(list(original_dist.keys()) + list(final_dist.keys()))
    for complexity in sorted(all_classes):
        orig_count = original_dist.get(complexity, 0)
        final_count = final_dist.get(complexity, 0)
        change = final_count - orig_count
        change_str = f"+{change}" if change > 0 else str(change)
        row_class = "improved" if change > 0 else ""
        
        html_content += f"""
                    <tr class="{row_class}">
                        <td>{complexity}</td>
                        <td>{orig_count:,}</td>
                        <td>{final_count:,}</td>
                        <td>{change_str}</td>
                    </tr>
        """
    
    html_content += """
                </table>
            </div>
    """
    
    # Add duplicate removal details if any
    if total_duplicates_removed > 0:
        html_content += f"""
            <div class="stats-box">
                <h4>ðŸ” Data Quality - Duplicate Removal Details</h4>
                <p>Total duplicates removed: <strong>{total_duplicates_removed:,}</strong></p>
                <ul>
                    <li><strong>Intra-split duplicates:</strong> {intra_duplicates_removed:,} (duplicates within same split)</li>
                    <li><strong>Cross-split leakage:</strong> {cross_duplicates_removed:,} (duplicates across train/val/test)</li>
                </ul>
                <table>
                    <tr>
                        <th>Split</th>
                        <th>Before Deduplication</th>
                        <th>After Deduplication</th>
                        <th>Removed</th>
                    </tr>
        """
        
        original_train = leakage_stats.get('original_train_size', merge_stats.get('train', 0))
        original_val = leakage_stats.get('original_val_size', merge_stats.get('val', 0))
        original_test = leakage_stats.get('original_test_size', merge_stats.get('test', 0))
        
        train_removed = leakage_stats.get('total_removed', {}).get('train', 0)
        val_removed = leakage_stats.get('total_removed', {}).get('val', 0)
        test_removed = leakage_stats.get('total_removed', {}).get('test', 0)
        
        html_content += f"""
                    <tr>
                        <td>Train</td>
                        <td>{original_train:,}</td>
                        <td>{final_train_size:,}</td>
                        <td>{train_removed:,}</td>
                    </tr>
                    <tr>
                        <td>Validation</td>
                        <td>{original_val:,}</td>
                        <td>{final_val_size:,}</td>
                        <td>{val_removed:,}</td>
                    </tr>
                    <tr>
                        <td>Test</td>
                        <td>{original_test:,}</td>
                        <td>{final_test_size:,}</td>
                        <td>{test_removed:,}</td>
                    </tr>
                </table>
            </div>
        """
    
    html_content += """
            <div class="stats-box">
                <h4>ðŸ“ Generated Files</h4>
                <ul>
                    <li><code>/opt/airflow/data/train.csv</code> - Final training dataset with sql_complexity column</li>
                    <li><code>/opt/airflow/data/val.csv</code> - Final validation dataset with sql_complexity column</li>
                    <li><code>/opt/airflow/data/test.csv</code> - Final test dataset with sql_complexity column</li>
                    <li><code>/opt/airflow/data/synthetic_data.csv</code> - Generated synthetic samples</li>
                    <li><code>/opt/airflow/data/raw_schema_and_stats.json</code> - Raw data schema and baseline statistics</li>
                    <li><code>/opt/airflow/data/engineered_schema_and_stats.json</code> - Engineered features schema and final statistics</li>
    """
    
    if validate_stats.get('total_anomalies', 0) > 0:
        html_content += """
                    <li><code>/opt/airflow/data/sql_validation_anomalies.csv</code> - Invalid SQL queries report</li>
        """
    
    html_content += """
                </ul>
            </div>
            
            <div class="stats-box">
                <h4>âœ¨ Key Achievements</h4>
                <ul>
                    <li>âœ… Successfully loaded and validated SQL dataset</li>
                    <li>âœ… <strong>Raw data schema validated</strong> with comprehensive type checking and baseline statistics</li>
                    <li>âœ… <strong>Statistical profiling</strong>: min/max/median/percentiles calculated for all text columns</li>
    """
    
    if validate_stats.get('total_anomalies', 0) > 0:
        html_content += f"""
                    <li>âœ… Identified and documented {validate_stats.get('total_anomalies', 0)} SQL anomalies</li>
        """
    
    if synthetic_stats.get('synthetic_generated', 0) > 0:
        html_content += f"""
                    <li>âœ… Generated {synthetic_stats.get('synthetic_generated', 0):,} synthetic samples for bias mitigation</li>
                    <li>âœ… Balanced class distribution from {bias_info.get('imbalance_ratio', 0):.2f}x to improved ratio</li>
                    <li>âœ… <strong>Synthetic data quality validated</strong> against original data statistics</li>
        """
    
    if total_duplicates_removed > 0:
        html_content += f"""
                    <li>âœ… <strong>Removed {total_duplicates_removed:,} duplicates</strong> ({intra_duplicates_removed:,} intra-split + {cross_duplicates_removed:,} cross-split)</li>
                    <li>âœ… <strong>Data leakage prevented</strong>: eliminated all cross-split duplicates</li>
        """
    
    exec_date = str(context.get('execution_date', 'N/A'))
    
    html_content += f"""
                    <li>âœ… <strong>Feature engineering applied</strong>: created input_text column with validated format</li>
                    <li>âœ… <strong>Engineered features schema validated</strong> with comprehensive checks</li>
                    <li>âœ… <strong>Data quality checks</strong>: validated data types, null values, empty strings, duplicates</li>
                    <li>âœ… <strong>SQL complexity column preserved</strong> in all final datasets for downstream tasks</li>
                    <li>âœ… Created stratified train/validation split ({final_train_size:,} train, {final_val_size:,} val)</li>
                    <li>âœ… <strong>Character and word statistics</strong> computed for all text columns</li>
                    <li>âœ… All datasets ready for model training with guaranteed schema compliance</li>
                </ul>
            </div>
            
            <p style="margin-top: 30px; color: #7f8c8d; font-size: 12px;">
                <em>Pipeline completed successfully on {exec_date}</em><br>
                <em>This is an automated success notification from the QueryHub Data Pipeline</em><br>
                <em>Schema validation ensures data quality and reproducibility across pipeline runs</em><br>
                <em><strong>Final dataset sizes reflect all processing steps including deduplication</strong></em>
            </p>
        </div>
    </body>
    </html>
    """
    
    # Send success notification
    send_email_notification(subject, html_content)
    
    logging.info("âœ… Pipeline success notification sent")
    logging.info(f"   Final dataset sizes - Train: {final_train_size:,}, Val: {final_val_size:,}, Test: {final_test_size:,}")
    if total_duplicates_removed > 0:
        logging.info(f"   Total duplicates removed: {total_duplicates_removed:,}")
    
    return {
        'notification_sent': True,
        'final_train_size': final_train_size,
        'final_val_size': final_val_size,
        'final_test_size': final_test_size,
        'total_duplicates_removed': total_duplicates_removed
    }


def upload_to_gcp(**context):
    logging.info("=" * 60)
    logging.info("UPLOADING DATASETS TO GCP")
    logging.info("=" * 60)

    # FIX: Use new-style Variable.get()
    bucket_name = Variable.get("GCS_BUCKET_NAME", default_var="text-to-sql-dataset-queryhub")
    project_id = Variable.get("gcp_project")

    logging.info(f"Bucket: {bucket_name}, Project: {project_id}")

    # FIX: Add connection fallback and better error handling
    try:
        # Try to use the GCP connection
        hook = GCSHook(gcp_conn_id="google_cloud_default")
        client = hook.get_conn()
        
    except Exception as conn_error:
        logging.error(f"âŒ GCP Connection failed: {conn_error}")
        logging.info("ðŸ”„ Attempting to create GCS client without Airflow connection...")
        
        # Fallback: Try to create client directly if credentials are available
        try:
            from google.cloud import storage
            # This will use Application Default Credentials
            # Make sure GOOGLE_APPLICATION_CREDENTIALS is set or credentials are available
            client = storage.Client()
        except Exception as fallback_error:
            logging.error(f"âŒ Fallback GCS client also failed: {fallback_error}")
            logging.error("Please set up GCP credentials or configure the Airflow connection")
            raise
    
    bucket = client.bucket(bucket_name)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = "/opt/airflow/data"
    uploaded_files = []

    # -------------------------
    # Upload dataset files
    # -------------------------
    for filename in ["train.csv", "val.csv", "test.csv"]:
        local_path = f"{output_dir}/{filename}"

        if not os.path.exists(local_path):
            logging.error(f"âŒ File not found: {local_path}")
            continue

        gcs_path = f"processed_datasets/{timestamp}/{filename}"
        blob = bucket.blob(gcs_path)
        blob.upload_from_filename(local_path)

        file_size = os.path.getsize(local_path)
        logging.info(f"   âœ… Uploaded {filename} â†’ gs://{bucket_name}/{gcs_path}")

        uploaded_files.append({
            "local": filename,
            "gcs_path": f"gs://{bucket_name}/{gcs_path}",
            "size_kb": file_size / 1024
        })

    # -------------------------
    # Upload metadata files
    # -------------------------
    metadata_files = [
        "raw_schema_and_stats.json",
        "engineered_schema_and_stats.json",
        "sql_validation_anomalies.csv",
        "synthetic_data.csv"
    ]

    for schema_file in metadata_files:
        local_path = f"{output_dir}/{schema_file}"

        if os.path.exists(local_path):
            gcs_path = f"processed_datasets/{timestamp}/{schema_file}"
            blob = bucket.blob(gcs_path)
            blob.upload_from_filename(local_path)

            file_size = os.path.getsize(local_path)
            logging.info(f"   âœ… Uploaded {schema_file} â†’ gs://{bucket_name}/{gcs_path}")

            uploaded_files.append({
                "local": schema_file,
                "gcs_path": f"gs://{bucket_name}/{gcs_path}",
                "size_kb": file_size / 1024
            })

    # -------------------------
    # Manifest file
    # -------------------------
    manifest_data = {
        "timestamp": timestamp,
        "folder": f"processed_datasets/{timestamp}/",
        "files": uploaded_files,
        "total_files": len(uploaded_files)
    }

    manifest_blob = bucket.blob(f"processed_datasets/{timestamp}/manifest.json")
    manifest_blob.upload_from_string(
        json.dumps(manifest_data, indent=2),
        content_type="application/json"
    )

    logging.info(f"ðŸ“‹ Manifest uploaded: gs://{bucket_name}/processed_datasets/{timestamp}/manifest.json")

    # -------------------------
    # Latest pointer
    # -------------------------
    latest_data = {
        "latest_run": timestamp,
        "folder": f"processed_datasets/{timestamp}/",
        "uploaded_at": datetime.now().isoformat()
    }

    latest_blob = bucket.blob("processed_datasets/latest.json")
    latest_blob.upload_from_string(
        json.dumps(latest_data, indent=2),
        content_type="application/json"
    )

    logging.info(f"ðŸ“Œ Latest pointer updated: gs://{bucket_name}/processed_datasets/latest.json")

    # Push XCom
    task_instance = context["task_instance"]
    task_instance.xcom_push(key="gcs_uploads", value=uploaded_files)
    task_instance.xcom_push(key="gcs_timestamp_folder", value=timestamp)

    logging.info("=" * 60)
    logging.info(f"GCP UPLOAD COMPLETE - Folder: {timestamp}")
    logging.info("=" * 60)

    logging.info(f"ðŸš€ Triggered downstream DAG: your_downstream_dag_id")

    return {
        "uploaded": len(uploaded_files),
        "timestamp": timestamp,
        "bucket": bucket_name,
        "folder": f"processed_datasets/{timestamp}/",
        "files": uploaded_files
    }
# ============================================================================
# DAG DEFINITION
# ============================================================================

dag = DAG(
    'data_pipeline_with_synthetic_v1_schema_validation',
    default_args=default_args,
    description='Text-to-SQL Data Ingestion Pipeline with Schema Validation, Synthetic Data Augmentation & Bias Detection',
    schedule=None,
    catchup=False,
    tags=['data-pipeline', 'synthetic-generation', 'bias-detection', 'schema-validation'],
)

t0_tests = PythonOperator(
    task_id='run_pytest_tests',
    python_callable=run_pytest_tests,
    dag=dag
)

t1 = PythonOperator(
    task_id='load_data_from_gcs',
    python_callable=load_data_from_gcs,
    dag=dag
)

t2 = PythonOperator(
    task_id='validate_sql',
    python_callable=validate_sql,
    dag=dag
)

t3 = PythonOperator(
    task_id='preprocess',
    python_callable=preprocess,
    dag=dag
)

t3a = PythonOperator(
    task_id='validate_raw_schema',
    python_callable=validate_raw_schema,
    dag=dag
)

t4 = PythonOperator(
    task_id='detect_bias',
    python_callable=detect_bias,
    dag=dag
)

t5 = PythonOperator(
    task_id='analyze_and_generate_synthetic',
    python_callable=analyze_and_generate_synthetic,
    dag=dag
)

t6 = PythonOperator(
    task_id='merge_and_split',
    python_callable=merge_and_split,
    dag=dag
)

t6a = PythonOperator(
    task_id='remove_data_leakage',
    python_callable=remove_data_leakage,
    dag=dag
)

t6b = PythonOperator(
    task_id='final_data_validation_check',
    python_callable=final_data_validation_check,
    dag=dag
)

t6c = PythonOperator(
    task_id='validate_engineered_schema',
    python_callable=validate_engineered_schema,
    dag=dag
)

compare = PythonOperator(
    task_id='compare_datasets',
    python_callable=compare_datasets,
    dag=dag
)

check_drift_branch = BranchPythonOperator(
    task_id='check_data_drift',
    python_callable=check_data_drift,
    dag=dag
)

t6d = PythonOperator(
    task_id='upload_to_gcp',
    python_callable=upload_to_gcp,
    dag=dag,
    trigger_rule='none_failed_min_one_success',
)

trigger_training = TriggerDagRunOperator(
    task_id='trigger_vertex_ai_training',
    trigger_dag_id='vertex_ai_model_training_pipeline',
    conf={
        "gcs_folder": "{{ ti.xcom_pull(task_ids='upload_to_gcp', key='gcs_folder') }}"
    },
    wait_for_completion=False,
    trigger_rule='none_failed_min_one_success',
)

skip_upload = PythonOperator(
    task_id='skip_upload',
    python_callable=skip_upload,
    trigger_rule='none_failed_min_one_success',
    dag=dag
)

t7 = PythonOperator(
    task_id='send_success_notification',
    python_callable=send_pipeline_success_notification,
    trigger_rule='none_failed_min_one_success',
    dag=dag
)

# t0_tests >> t1 >> t2 >> t3 >> t3a >> t4 >> t5 >> t6 >> t6a >> t6b >> t6c >> compare >> check_data_drift >> t6d >> trigger_training >> skip_upload >> t7

# Update your task dependencies:
t0_tests >> t1 >> t2 >> t3 >> t3a >> t4 >> t5 >> t6 >> t6a >> t6b >> t6c >> compare >> check_drift_branch

# Branch paths
check_drift_branch >> [t6d, skip_upload]

# Continue from upload path
t6d >> trigger_training >> t7

# Continue from skip path  
skip_upload >> t7